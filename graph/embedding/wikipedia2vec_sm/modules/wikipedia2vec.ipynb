{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for demonstrating Wikipedia2Vec: A tool for learning vector representations of words and entities from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pkg_resources\n",
    "import io\n",
    "import numpy as np\n",
    "from tempfile import NamedTemporaryFile\n",
    "from wikipedia2vec.dump_db import DumpDB\n",
    "from wikipedia2vec.dictionary import Dictionary, Item, Word, Entity\n",
    "from wikipedia2vec.link_graph import LinkGraph\n",
    "from wikipedia2vec.mention_db import MentionDB\n",
    "from wikipedia2vec.wikipedia2vec import Wikipedia2Vec\n",
    "from wikipedia2vec.utils.wiki_dump_reader import WikiDumpReader\n",
    "from wikipedia2vec.utils.tokenizer import get_tokenizer, get_default_tokenizer\n",
    "from wikipedia2vec.utils.sentence_detector import get_sentence_detector\n",
    "from Wikipedia2vec import Wikipedia2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read the dataset\n",
    "Reads a dataset in preparation to learn embeddings. Returns data in proper format to learn embeddings. Saves the required file names in chosen_dataset.txt to be used by other methods of the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    train = np.genfromtxt(\"data/Yago/train.txt\", delimiter='\\t', dtype='str', usecols=np.arange(0,3))\n",
    "    valid = np.genfromtxt(\"data/Yago/valid.txt\", delimiter='\\t', dtype='str', usecols=np.arange(0,3))\n",
    "    test = np.genfromtxt(\"data/Yago/test.txt\", delimiter='\\t', dtype='str', usecols=np.arange(0,3))\n",
    "    entity2id = np.genfromtxt(\"data/Yago/entity2id.txt\", delimiter='\\t', dtype='str', usecols=np.arange(0,2))\n",
    "    relation2id = np.genfromtxt(\"data/Yago/relation2id.txt\", delimiter='\\t', dtype='str', usecols=np.arange(0,2))\n",
    "    file_name = 'yago.txt'\n",
    "    with open('chosen_dataset.txt', 'w') as the_file:\n",
    "        the_file.write(file_name)\n",
    "    the_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Dump Database\n",
    "Wikipedia2vec dump can be obtained using wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2. It takes around 2 hours to download.\n",
    "\n",
    "The build_dump command creates a database that contains Wikipedia pages each of which consists of texts and anchor links in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dump(dump_file, out_file):\n",
    "    dump_reader = WikiDumpReader(dump_file)\n",
    "    DumpDB.build(dump_reader, out_file, pool_size=multiprocessing.cpu_count(), chunk_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_file = 'enwiki-latest-pages-articles.xml.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dump(dump_file, 'output.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Dictionary\n",
    "\n",
    "The build_dictionary command builds a dictionary of words and entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(dump_db_file, out_file):\n",
    "    dump_db = DumpDB(dump_db_file)\n",
    "    dictionary = Dictionary.build(\n",
    "        dump_db=dump_db,\n",
    "        tokenizer=get_default_tokenizer(dump_db.language),\n",
    "        category=False,\n",
    "        lowercase= True,\n",
    "        min_entity_count=5,\n",
    "        min_paragraph_len=5,\n",
    "        pool_size=multiprocessing.cpu_count(),\n",
    "        disambi=False,\n",
    "        chunk_size=100,\n",
    "        min_word_count=5)\n",
    "    dictionary.save(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dictionary('output.db', 'output_dic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Mention DB \n",
    "\n",
    "The build_mention_db command builds a database that contains the mappings of entity names (mentions) and their possible referent entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mention_db(dump_db_file, dictionary_file, out_file):\n",
    "    dump_db = DumpDB(dump_db_file)\n",
    "    dictionary = Dictionary.load(dictionary_file)\n",
    "    mention_db = MentionDB.build(dump_db, dictionary,\n",
    "        tokenizer=get_default_tokenizer(dump_db.language),\n",
    "        min_link_prob=0.2,\n",
    "        min_prior_prob=0.01,\n",
    "        pool_size=multiprocessing.cpu_count(),\n",
    "        max_mention_len=20,\n",
    "        chunk_size=100,\n",
    "        case_sensitive=False)\n",
    "    mention_db.save(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_mention_db('output.db', 'output_dic', 'output_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Link Graph\n",
    "\n",
    "The build_link_graph command generates a sparse matrix representing the link structure between Wikipedia entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_link_graph(dump_db_file, dictionary_file, out_file):\n",
    "    dump_db = DumpDB(dump_db_file)\n",
    "    dictionary = Dictionary.load(dictionary_file)\n",
    "    link_graph = LinkGraph.build(dump_db, dictionary, pool_size=multiprocessing.cpu_count(), chunk_size=100)\n",
    "    link_graph.save(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24000/24000 [00:03<00:00, 6363.71it/s]\n"
     ]
    }
   ],
   "source": [
    "build_link_graph('output.db', 'output_dic', 'output_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learn Embeddings\n",
    "\n",
    "The learn_embeddings command runs the training of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_embeddings(dump_db_file, dictionary_file, out_file, link_graph_file=None, mention_db_file=None):\n",
    "    dump_db = DumpDB(dump_db_file)\n",
    "    dictionary = Dictionary.load(dictionary_file)\n",
    "\n",
    "    link_graph = LinkGraph.load(link_graph_file, dictionary) if link_graph_file else None\n",
    "    mention_db = MentionDB.load(mention_db_file, dictionary) if mention_db_file else None\n",
    "\n",
    "    wiki2vec = Wikipedia2Vec(dictionary)\n",
    "    wiki2vec.train(dump_db, link_graph, mention_db,\n",
    "        tokenizer=get_default_tokenizer(dump_db.language),\n",
    "        sentence_detector=None,\n",
    "        entity_neg_power=0.0,\n",
    "        entities_per_page=10,\n",
    "        dim_size=100,\n",
    "        iteration=5,\n",
    "        negative=5,\n",
    "        pool_size=multiprocessing.cpu_count(),\n",
    "        sample=0.0001,\n",
    "        window=5,\n",
    "        chunk_size=100,\n",
    "        init_alpha=0.025,\n",
    "        min_alpha=0.0001,\n",
    "        word_neg_power=0.75)\n",
    "\n",
    "    wiki2vec.save(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9330b1fc6625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output.db'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output_dic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'final_output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_graph_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output_lg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmention_db_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7f67ff2ecfda>\u001b[0m in \u001b[0;36mlearn_embeddings\u001b[0;34m(dump_db_file, dictionary_file, out_file, link_graph_file, mention_db_file)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0minit_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         word_neg_power=0.75)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mwiki2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/wikipedia2vec/wikipedia2vec.pyx\u001b[0m in \u001b[0;36mwikipedia2vec.wikipedia2vec.Wikipedia2Vec.train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mword_neg_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_word_neg_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_neg_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mentity_neg_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_entity_neg_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_neg_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building tables for link indices...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/wikipedia2vec/wikipedia2vec.pyx\u001b[0m in \u001b[0;36mwikipedia2vec.wikipedia2vec.Wikipedia2Vec._build_entity_neg_table\u001b[0;34m()\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_entity_neg_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat32_t\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpower\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_uniform_neg_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_unigram_neg_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/wikipedia2vec/wikipedia2vec.pyx\u001b[0m in \u001b[0;36mwikipedia2vec.wikipedia2vec.Wikipedia2Vec._build_uniform_neg_table\u001b[0;34m()\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mcdef\u001b[0m \u001b[0mItem\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRawArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_int32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_unigram_neg_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat32_t\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint32_t\u001b[0m \u001b[0mtable_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/wikipedia2vec/dictionary.pyx\u001b[0m in \u001b[0;36mentities\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mcdef\u001b[0m \u001b[0mint32_t\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entity_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mEntity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entity_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entity_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/six.pyc\u001b[0m in \u001b[0;36miteritems\u001b[0;34m(d, **kw)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_embeddings('output.db', 'output_dic', 'final_output', link_graph_file='output_lg', mention_db_file='output_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model\n",
    "\n",
    "The save_model command outputs the model in a text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_file, out_file, out_format='default'):\n",
    "    wiki2vec = Wikipedia2Vec.load(model_file)\n",
    "    wiki2vec.save_text(out_file, out_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('final_output', 'final_output_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Model\n",
    "\n",
    "The load_model command is used to load a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Constitution Party (United States)',\n",
       "        '1.4314 0.6509 0.1369 -0.2897 0.9381 -0.7960 -0.2041 0.1075 -0.0845 -0.9855 1.0325 2.1194 0.0237 0.2636 0.2425 1.0091 0.5908 0.7337 1.0987 -0.5816 1.5382 0.0214 -0.6255 0.2021 -1.5465 0.6726 -0.0227 0.0372 -0.7585 -0.1627 0.5472 -0.2056 0.6648 -0.3341 -0.7990 0.7560 0.6875 -1.2692 -0.8705 0.3443 -0.9969 0.8561 -0.2194 1.7175 1.6140 -1.2135 -0.0415 0.9827 1.0512 -1.0306 -0.3996 0.6865 -0.3193 0.3552 -0.0496 1.5362 0.4726 -1.1567 -0.1246 -0.4665 0.5021 0.5424 -0.6211 -0.0685 -1.1872 -0.0262 0.1155 -0.5210 -0.6172 1.2290 -0.3012 -0.1781 -0.1648 -0.2855 1.1114 0.1631 0.5178 0.2111 -0.6711 -0.2107 1.5018 0.6680 1.2189 0.4314 0.4679 0.0888 -0.3454 0.4530 0.5873 0.2863 -0.1899 0.2594 0.3325 0.1133 0.2565 0.7823 -0.7421 0.0873 0.0167 1.3978'],\n",
       "       ['Charles Kennedy',\n",
       "        '0.8849 0.2273 -0.0471 0.1152 0.7337 -0.4782 -0.5643 -0.8389 -0.4170 -0.8331 1.6839 1.3857 0.0676 -0.0253 0.5190 0.2236 0.0446 0.3481 0.5023 0.1022 -0.0678 -0.6835 -1.0881 -0.5297 -0.8817 -0.0178 -0.6935 -0.0209 -0.3202 1.1328 0.2189 -1.0589 0.5469 -0.2370 0.4949 0.3466 0.7067 -0.9515 0.0587 -0.1295 -0.4921 0.7394 0.2187 0.7132 1.5649 -0.1625 0.1818 -0.0054 1.4425 0.5437 0.3483 0.2418 -1.9312 0.5471 -0.0128 0.4161 -0.5488 -0.7209 -0.5858 -0.8763 -0.1765 0.1762 -0.5307 -0.1709 -1.1794 -0.3438 0.0574 0.4176 0.2337 0.9680 0.1348 0.5809 0.9108 0.7661 1.1812 -0.7986 -0.8788 0.0781 -0.8124 -0.0362 -0.4554 0.7575 0.6684 -1.0339 -0.4700 -0.2442 -0.0895 0.7281 -0.6826 0.6316 -0.2405 -0.8388 -0.1954 1.1102 -0.8773 0.5690 0.1583 -0.1837 0.4874 0.1135'],\n",
       "       ['Landon Donovan',\n",
       "        '-0.7269 -0.0265 -0.4549 0.5591 1.1092 -0.3809 1.4216 0.0457 0.2069 -0.4540 -1.0169 -0.0141 -0.8930 -0.2071 1.0363 -0.0362 0.1917 0.0037 0.8070 0.5086 0.9293 -0.3472 -0.7872 -0.7399 0.1977 0.7709 -0.9507 0.3664 0.1896 0.8007 0.1021 0.4537 1.1413 -0.6541 -0.2374 1.0378 0.9750 0.1905 0.6397 -0.2618 0.4123 -0.3542 1.1698 1.4076 1.3041 -0.6344 -0.0960 1.0128 -0.4371 0.0968 -0.9653 0.6495 -1.6071 -0.3571 0.1279 1.0945 -1.3101 -0.0192 0.2067 0.2309 0.8659 0.5255 -0.4463 -0.3712 0.0553 -0.1320 -0.7572 0.1167 -0.6716 0.2437 -0.3068 0.2298 0.1635 0.6867 0.2393 0.3922 -1.2909 1.1447 -1.0289 -1.3792 0.7582 -0.4814 1.0742 -0.8813 -0.7038 -0.3715 -2.1286 0.2651 0.0302 0.4598 0.4128 -0.2813 -0.5055 0.0059 0.5057 0.7704 0.4116 -1.8733 0.0974 1.9321'],\n",
       "       ...,\n",
       "       ['Baltimore',\n",
       "        '-0.2470 0.2755 -0.4118 0.5256 0.6352 -0.9644 0.8206 0.9467 -0.2933 -0.7248 -0.1516 -0.6707 -0.0945 0.3886 -0.5007 -0.0978 0.1127 -0.4835 0.8246 -0.7848 0.5385 0.4660 -0.6559 -1.5447 -0.1296 -1.2270 0.6592 -0.0396 -0.5357 0.0445 0.3096 -1.4337 -0.1075 -0.3682 0.2996 -0.2598 0.6229 0.0599 0.0842 -0.6694 0.7403 0.5319 0.3187 0.2963 0.3531 -0.2833 -0.1917 0.6009 0.9490 -0.7064 0.4750 0.2793 0.0996 -0.3299 0.1622 0.3046 -0.7674 0.3390 0.3096 -0.6890 -0.4708 -0.5144 -0.6897 -0.8752 0.5445 -0.3128 -0.2653 -0.6486 0.9263 0.2267 -0.4964 0.7346 -0.1291 -0.1988 1.8271 0.2014 -0.3016 0.4390 -1.0520 0.1834 0.6028 0.0786 -0.7461 1.2423 0.9558 0.0914 0.2380 -0.1213 0.4954 0.9987 0.0274 1.1796 0.2833 -0.2331 -0.6672 0.2335 0.8105 -1.6808 -0.4248 0.9629'],\n",
       "       ['Mount Vernon, New York',\n",
       "        '0.6776 -0.8625 -1.4248 0.2863 -0.1660 0.6437 1.6674 1.4555 0.5742 -0.4230 -0.3640 -0.1133 -0.7750 0.4699 -0.4264 0.8962 0.3822 0.5130 0.2528 -0.4945 1.5247 1.3078 0.1109 -1.4591 -1.2123 0.3884 0.0065 -0.6949 -1.9356 0.4451 -0.6318 -0.3076 -0.0264 -0.0690 0.4348 -1.3623 0.5096 0.9166 0.3619 -2.4370 -0.3784 -0.7895 -0.7208 0.8508 1.0376 -0.1338 -0.6642 0.4247 0.3906 -0.3587 0.1556 0.1975 0.4954 0.6056 0.7872 1.0446 -0.5560 0.2891 0.0170 1.0466 1.4689 0.2580 -1.4878 0.4375 1.8890 0.8383 -0.3170 -0.7339 0.6107 -0.5112 -0.0328 1.6220 -1.1249 -0.4952 0.8441 0.5949 -0.0481 -0.2650 -0.1893 -1.3063 -0.1913 0.4656 -0.3052 0.4449 -0.1574 -1.0012 0.2052 0.5549 -1.3353 -0.2403 -0.8583 -0.1939 1.3480 1.3622 0.0140 0.2089 0.2596 -1.6934 -1.5247 1.8332'],\n",
       "       ['Stockholm',\n",
       "        '-1.1391 -0.5006 -0.1050 -0.2473 0.3556 -1.7490 0.2439 -0.5688 0.5490 -1.9069 0.1882 -1.4931 0.0971 -0.1172 -0.3644 1.0020 0.2971 -0.6425 0.9561 -0.2326 0.1082 -0.3591 -0.1658 -1.3896 0.3843 -1.2141 0.2974 -1.3497 0.4115 0.5075 -1.0433 -0.3273 0.3954 -0.1378 0.4979 -0.4685 0.4240 -0.7639 -0.0197 -0.5016 -0.8857 -0.1772 -0.3219 0.6998 1.5067 0.4500 0.0196 -1.0549 0.2425 -0.4760 0.1113 -0.3384 -0.7352 0.1063 0.3508 -0.2767 -0.4932 0.3514 0.1421 -0.2701 -0.2248 -1.3640 -0.0578 -0.5285 -0.5991 -0.4775 0.5673 -0.6071 0.8033 -0.3794 -0.5015 0.2262 -0.1917 0.1897 1.1823 -0.0070 0.2512 0.7745 -1.0730 0.2931 0.9860 -0.2139 -0.9500 0.8546 0.1495 0.5056 -0.0458 0.0821 1.5849 -0.0169 -0.7677 0.9061 -0.5492 1.2968 -0.8303 -1.1341 0.5517 -0.9927 -0.5338 1.3379']],\n",
       "      dtype='|S761')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = Wikipedia2vec()\n",
    "obj.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation\n",
    "\n",
    "Evaluation metric used is Spearman's coefficient using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6255656104786236"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
