{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_final import NeuralSequenceLabeler\n",
    "from utils import parseconfig\n",
    "from utils.conll2003_prepro import process_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = parseconfig.parseConfig()\n",
    "file_dict = {\"train\":os.path.join(config[\"raw_path\"], \"train1.txt\"),\"dev\":os.path.join(config[\"raw_path\"], \"valid1.txt\"),\"test\":os.path.join(config[\"raw_path\"], \"test1.txt\")}\n",
    "neuralSequenceLabeler =  NeuralSequenceLabeler(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate object of NeuralSequenceLabeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralSequenceLabeler =  NeuralSequenceLabeler(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split data/raw/benchmark_dataset/raw/train1.txt\n",
      "split data/raw/benchmark_dataset/raw/test1.txt\n",
      "split data/raw/benchmark_dataset/raw/valid1.txt\n",
      "inside read_dataset\n",
      "train 219554\n",
      "dev 55044\n",
      "test 50350\n"
     ]
    }
   ],
   "source": [
    "dataset = neuralSequenceLabeler.read_dataset(file_dict,\"ontonotes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inititalise metadata & preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embedding shape: [None, None, 300]\n",
      "chars representation shape: [None, None, 200]\n",
      "word and chars concatenation shape: [None, None, 500]\n",
      "rnn output shape: [None, None, 600]\n",
      "logits shape: [None, None, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aishwaryaadiga/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params number: 6039690\n"
     ]
    }
   ],
   "source": [
    "train_set,dev_set,test_set,vocab = process_data(dataset,config)\n",
    "neuralSequenceLabeler.initialize_metadata(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {\"train\": train_set, \"dev\":dev_set,\"test\":test_set }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in load model....\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    neuralSequenceLabeler.load_model()\n",
    "except:\n",
    "    print(\"Error loading the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14987\n",
      "3466\n",
      "3684\n"
     ]
    }
   ],
   "source": [
    "dataset = neuralSequenceLabeler.read_dataset_helper(file_dict,\"ontonotes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch 1/2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 605s - Global Step: 750 - Train Loss: 3.0879   \n",
      "\n",
      "\n",
      "in predict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev dataset -- pre: 76.35, rec: 79.21, FB1: 77.75\n",
      " -- new BEST score on test dataset: 77.75\n",
      "Epoch 2/2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 554s - Global Step: 1500 - Train Loss: 0.7879   \n",
      "\n",
      "\n",
      "in predict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev dataset -- pre: 81.73, rec: 83.85, FB1: 82.78\n",
      " -- new BEST score on test dataset: 82.78\n"
     ]
    }
   ],
   "source": [
    "neuralSequenceLabeler.train(dataset[\"train_set\"],dataset[\"dev_data\"],dataset[\"dev_set\"],dataset[\"test_set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "in predict...\n"
     ]
    }
   ],
   "source": [
    "predictions_formatted = neuralSequenceLabeler.predict(dataset[\"test_set\"],\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "groundTruths = []\n",
    "words_list = []\n",
    "for i in range(len(predictions_formatted)):\n",
    "    predictions_sentence = []\n",
    "    groundTruths_sentence = []\n",
    "    words_list_sentence = []\n",
    "    for j in range(len(predictions_formatted[i])):\n",
    "\n",
    "        words_list_sentence.append(predictions_formatted[i][j][2])\n",
    "        predictions_sentence.append(predictions_formatted[i][j][3])\n",
    "\n",
    "    predictions.append(predictions_sentence)\n",
    "\n",
    "    words_list.append(words_list_sentence)\n",
    "for data in dataset[\"test_set\"]:\n",
    "    for tags,  seq_len in zip(data[\"tags\"], data[\"seq_len\"]):\n",
    "            tags = [neuralSequenceLabeler.rev_tag_dict[x] for x in tags[:seq_len]]\n",
    "            groundTruths.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-PER', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "print(predictions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test dataset -- pre: 81.73, rec: 83.85, FB1: 82.78\n"
     ]
    }
   ],
   "source": [
    "save_path = os.path.join(neuralSequenceLabeler.cfg[\"checkpoint_path\"], \"result.txt\")\n",
    "name = \"test\"\n",
    "score = neuralSequenceLabeler.evaluate(predictions, groundTruths,words_list, save_path,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the predictions \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
