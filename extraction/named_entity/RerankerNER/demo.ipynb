{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from utils.alphabet import Alphabet\n",
    "from utils.data_processor import *\n",
    "from model.Model import *\n",
    "from utils.keras_utils import padding\n",
    "from utils.metric import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_alphabet = Alphabet('word')\n",
    "char_alphabet = Alphabet('char')\n",
    "\n",
    "nb_epoch = 1\n",
    "use_char = True\n",
    "mask_zero = True\n",
    "BILSTM = True\n",
    "DropProb = 0.2\n",
    "case_sense = True\n",
    "batch_size = 128\n",
    "grad_discent = \"adam\"\n",
    "lstm_average = False\n",
    "label_type = 'BMES'\n",
    "char_emb_dims = 50\n",
    "nb_filter = 100\n",
    "filter_length = 3\n",
    "lstm_hidden_dims = 100\n",
    "dense_hidden = 100\n",
    "model_dict = { \"A\": \"LSTM_model\", \"B\": \"LSTM_CNN_model\", \"C\":\"AttLSTM_model\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_file, dev_file, test_file, embedding_file):\n",
    "\tnp.random.seed(1337)\n",
    "\tcreate_alphabet([train_file, dev_file, test_file],word_alphabet, char_alphabet, True, True)\n",
    "\n",
    "\tpretrain_word_emb = match_embedding(embedding_file, word_alphabet)\n",
    "\ttrain_structure = load_rerank_data(train_file)\n",
    "\ttrain_word, train_char, train_label, train_mask = generate_tensor(train_structure, word_alphabet, char_alphabet, True, True, True)\n",
    "\tdev_structure = load_rerank_data(dev_file)\n",
    "\tdev_word, dev_char, dev_label, dev_mask = generate_tensor(dev_structure, word_alphabet, char_alphabet, True, True, False)\n",
    "\ttest_structure = load_rerank_data(test_file)\n",
    "\ttest_word, test_char, test_label, test_mask = generate_tensor(test_structure, word_alphabet, char_alphabet, True, True, False)\n",
    "\n",
    "\tprint \"Train instance:oracle:first =\", len(train_word),\":\", oracle_best_f(train_structure, 11, label_type), \":\", oracle_best_f(train_structure, 1, label_type)\n",
    "\tprint \"Dev instance:oracle:first =\", len(dev_word),\":\", oracle_best_f(dev_structure, 10, label_type), \":\", oracle_best_f(dev_structure, 1, label_type)\n",
    "\tprint \"Test instance:oracle:first =\",len(test_word), \":\",oracle_best_f(test_structure, 10, label_type), \":\", oracle_best_f(test_structure, 1, label_type)\n",
    "\n",
    "\treturn  train_word, train_char, train_label,\\\n",
    "\t\t\tdev_word, dev_char, dev_label,\\\n",
    "\t\t\ttest_word, test_char, test_label,\\\n",
    "\t\t\tpretrain_word_emb, \\\n",
    "\t\t\ttrain_structure,dev_structure,test_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_file=\"data/train.rerank.filter\", dev_file=\"data/dev.rerank.filter\", test_file=\"data/test.rerank.filter\", embedding_file=\"data/SENNA.emb\", TRAIN='train', FILE='DEBUG', MODEL_MODE='B', selected_iter = -1):\n",
    "\tprint (\"Setting summary: \")\n",
    "\tprint (\"\tMODEL_MODE: %s\\n\\\n",
    "\tUSE CHAR:%s\\n\\\n",
    "\tAVGPOOL:%s\\n\\\n",
    "\tembedding_file: %s\\n\\\n",
    "\tnb_epoch: %s\\n\\\n",
    "\tmask_zero: %s\\n\\\n",
    "\tBILSTM: %s\\n\\\n",
    "\tDropProb: %s\\n\\\n",
    "\tcase_sense: %s\\n\\\n",
    "\tgrad_discent: %s\\n\\\n",
    "\tlstm_hidden_dims:%s\\n\\\n",
    "\tnb_filter:%s\\n\\\n",
    "\tchar_emb_dims:%s\" % (model_dict[MODEL_MODE],use_char,lstm_average,embedding_file,nb_epoch,mask_zero,BILSTM,DropProb, case_sense, grad_discent, lstm_hidden_dims, nb_filter, char_emb_dims))\n",
    "\n",
    "\tif \"debug\" in FILE.lower():\n",
    "\t\tprint (\"IN DEBUG MODEL......  IF TRAINING: %s\" % TRAIN)\n",
    "\t\tDEBUG_MODEL = True\n",
    "\telse:\n",
    "\t\tprint (\"IN NORMAL MODEL...... IF TRAINING: %s\" % TRAIN)\n",
    "\t\tDEBUG_MODEL = False\n",
    "\n",
    "\n",
    "\tif DEBUG_MODEL:\n",
    "\t\ttrain_file = \"data/small_train.txt\"\n",
    "\t\tdev_file = \"data/small_dev.txt\"\n",
    "\t\ttest_file = \"data/small_test.txt\"\n",
    "\n",
    "\tprint \"\tTrain file:\", train_file\n",
    "\tprint \"\tDev file:\", dev_file\n",
    "\tprint \"\tTest file:\", test_file\n",
    "\n",
    "\tsys.stdout.flush()\t\n",
    "\tX_train,X1_train,Y_train,X_dev,X1_dev,Y_dev,X_test,X1_test,Y_test,word_pretrain_embedding,structure_train,structure_dev,structure_test = prepare_data(train_file,dev_file,test_file, embedding_file)\n",
    "\t# set parameters:\n",
    "\tword_vocab_size = word_alphabet.size()\n",
    "\tchar_vocab_size = char_alphabet.size()\n",
    "\n",
    "\tword_emb_dims = word_pretrain_embedding.shape[1]\n",
    "\t\n",
    "\tword_max_len = X1_train.shape[1]\n",
    "\tchar_max_len = X1_train.shape[2]\n",
    "\n",
    "\t\n",
    "\tembedding_name = embedding_file.split('/')[-1]\n",
    "\tFILE_NAME = \"./results/\" +\"MODEL.\"+MODEL_MODE+\"_CHAR.\"+str(use_char)+\"_DB.\"+str(DEBUG_MODEL)+\"_Mask.\"+str(mask_zero)+\"_BILSTM.\"+str(BILSTM) + \"_Emb.\" + embedding_name + \"_drop.\" + str(DropProb) +\"_GD.\"+grad_discent+ \"_Iter.\"\n",
    "\n",
    "\tdef my_init(shape, dtype=None):\n",
    "\t\treturn word_pretrain_embedding\n",
    "\tif MODEL_MODE == 'A':\n",
    "\t\tmodel = LSTM_model(use_char, word_vocab_size, word_max_len, char_vocab_size, char_max_len,  word_emb_dims, char_emb_dims, lstm_hidden_dims, nb_filter, filter_length, dense_hidden, my_init,mask_zero,BILSTM,DropProb,lstm_average,grad_discent)\n",
    "\telif MODEL_MODE == 'B':\n",
    "\t\tmodel = LSTM_CNN_model(use_char, word_vocab_size, word_max_len, char_vocab_size, char_max_len,  word_emb_dims, char_emb_dims, lstm_hidden_dims, nb_filter, filter_length, dense_hidden, my_init,mask_zero,BILSTM,DropProb,lstm_average,grad_discent)\n",
    "\telif MODEL_MODE == 'C':\n",
    "\t\tmodel = AttLSTM_model(use_char, word_vocab_size, word_max_len, char_vocab_size, char_max_len,  word_emb_dims, char_emb_dims, lstm_hidden_dims, nb_filter, filter_length, dense_hidden, my_init,mask_zero,BILSTM,DropProb,lstm_average,grad_discent)\n",
    "\telse:\n",
    "\t\tprint \"ERROR MODEL MODE:\", MODEL_MODE\n",
    "\t# plot_model(model, to_file=MODEL_MODE+\".BI.\"+str(BILSTM)+\".\"+model_dict[MODEL_MODE]+'.png',show_shapes=True)\n",
    "\tif use_char:\n",
    "\t\ttrain_in_dict = {\"word_input\":X_train,\"char_input\":X1_train}\n",
    "\t\tdev_in_dict = {\"word_input\":X_dev,\"char_input\":X1_dev}\n",
    "\t\ttest_dict = {\"word_input\":X_test,\"char_input\":X1_test}\n",
    "\telse:\n",
    "\t\ttrain_in_dict = {\"word_input\":X_train}\n",
    "\t\tdev_in_dict = {\"word_input\":X_dev}\n",
    "\t\ttest_dict = {\"word_input\":X_test}\n",
    "\ttrain_out_dict = {\"output\":Y_train}\n",
    "\tdev_out_dict = {\"output\":Y_dev}\n",
    "\n",
    "\n",
    "\tif TRAIN.lower()== \"train\":\n",
    "\t\tprint (\"Start to train model in normal step......\")\n",
    "\t\tcheckpointer = ModelCheckpoint(filepath= FILE_NAME+\"{epoch:02d}.hdf5\", verbose=1, save_best_only=False, mode='auto')\n",
    "\t\tmodel.fit(train_in_dict, train_out_dict, shuffle=True, epochs=nb_epoch, batch_size=batch_size, callbacks=[checkpointer], validation_data=(dev_in_dict, dev_out_dict) )\n",
    "\t\n",
    "\t## development data using all saved model\n",
    "\tbest_f = -1\n",
    "\tbest_epoch = -1\n",
    "\tbest_accuracy = -1\n",
    "\talpha_step = 0.005\n",
    "\talpha_num = int(1/alpha_step)\n",
    "\tbest_alpha = -1.0\n",
    "\tbest_f = -1.0\n",
    "\tbest_accuracy = -1.0\n",
    "\n",
    "\tprint (\"Start to load existing model %s......\" % FILE_NAME)\n",
    "\tfor idx in range(0, nb_epoch):\n",
    "\t\tif selected_iter >= 0:\n",
    "\t\t\tif idx != selected_iter:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tprint \"Select iteration:\", selected_iter\n",
    "\n",
    "\t\tmodel_name = FILE_NAME + str(idx).zfill(2) + \".hdf5\"\n",
    "\t\tmodel.load_weights(model_name)\n",
    "\t\tpredict_dev = model.predict(dev_in_dict, batch_size, 0)\n",
    "\t\tepoch_accuracy = -1\n",
    "\t\tepoch_p = -1 \n",
    "\t\tepoch_r = -1\n",
    "\t\tepoch_f = -1\n",
    "\t\tepoch_alpha = -1\n",
    "\t\tfor idy in range(alpha_num):\n",
    "\t\t\talpha = idy * alpha_step\n",
    "\t\t\tgolden_list, predict_choose_list = get_alpha_golden_predict_choose_results(structure_dev, predict_dev, alpha)\n",
    "\t\t\taccuracy = candidate_choose_accuracy(golden_list, predict_choose_list)\n",
    "\t\t\tp,r,f = get_rerank_ner_fmeasure(structure_dev,predict_choose_list,label_type)\n",
    "\t\t\tif f > epoch_f:\n",
    "\t\t\t\tepoch_f = f\n",
    "\t\t\t\tepoch_accuracy = accuracy\n",
    "\t\t\t\tepoch_p = p\n",
    "\t\t\t\tepoch_r = r\n",
    "\t\t\t\tepoch_alpha = alpha\n",
    "\t\t\t\torigin_dev_file = FILE_NAME+'origindev'\n",
    "\t\t\t\tsave_predict_result(structure_dev, predict_choose_list, origin_dev_file)\n",
    "\t\tprint (\"epoch: %s; alpha:%s; best f: %s; choose accuracy:%s\" % (idx, epoch_alpha, epoch_f, epoch_accuracy))\n",
    "\t\tif epoch_f > best_f:\n",
    "\t\t\tbest_accuracy = epoch_accuracy\n",
    "\t\t\tbest_alpha = epoch_alpha\n",
    "\t\t\tbest_epoch = idx\n",
    "\t\t\tbest_f = epoch_f\n",
    "\t\t\tbest_p = epoch_p\n",
    "\t\t\tbest_r = epoch_r\n",
    "\tif selected_iter >= 0:\n",
    "\t\tprint (\"Fix epoch/best alpha: %s/%s; P/R/F: %s/%s/%s;  choose accuracy: %s\"% (best_epoch,best_alpha, best_p, best_r, best_f, best_accuracy))\n",
    "\telse:\n",
    "\t\tprint (\"Best epoch/alpha: %s/%s; P/R/F: %s/%s/%s;  choose accuracy: %s\"% (best_epoch,best_alpha, best_p, best_r, best_f, best_accuracy))\n",
    "\n",
    "\t\n",
    "\tmodel_name = FILE_NAME + str(best_epoch).zfill(2) + \".hdf5\"\n",
    "\tprint (\"Loading model: %s\" % model_name)\n",
    "\tmodel.load_weights(model_name)\n",
    "\n",
    "\tpredict_test = model.predict(test_dict, 32, 0)\n",
    "\tgolden_list, predict_choose_list = get_alpha_golden_predict_choose_results(structure_test, predict_test, best_alpha)\n",
    "\ttest_accuracy = candidate_choose_accuracy(golden_list, predict_choose_list)\n",
    "\tp,r,f = get_rerank_ner_fmeasure(structure_test,predict_choose_list,label_type)\n",
    "\tprint (\"Test data: P: %s , R: %s, F: %s, Accuracy: %s\" % (p,r,f,test_accuracy))\n",
    "\t## save test result\n",
    "\tresult_name = model_name.split('.hdf5')[0]\n",
    "\tsave_predict_result(structure_test,predict_choose_list, result_name+'.test')\n",
    "\t## save dev result\n",
    "\tgolden_list, predict_choose_list = get_alpha_golden_predict_choose_results(structure_dev, predict_dev, best_alpha)\n",
    "\tsave_predict_result(structure_dev,predict_choose_list, result_name+'.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting summary: \n",
      "\tMODEL_MODE: LSTM_CNN_model\n",
      "\tUSE CHAR:True\n",
      "\tAVGPOOL:False\n",
      "\tembedding_file: data/SENNA.emb\n",
      "\tnb_epoch: 1\n",
      "\tmask_zero: True\n",
      "\tBILSTM: True\n",
      "\tDropProb: 0.2\n",
      "\tcase_sense: True\n",
      "\tgrad_discent: adam\n",
      "\tlstm_hidden_dims:100\n",
      "\tnb_filter:100\n",
      "\tchar_emb_dims:50\n",
      "IN DEBUG MODEL......  IF TRAINING: train\n",
      "\tTrain file: data/small_train.txt\n",
      "\tDev file: data/small_dev.txt\n",
      "\tTest file: data/small_test.txt\n",
      "Create alphabets, case_sense:True, norm_digit:True\n",
      "Word Alphabet size: 1807\n",
      "Char Alphabet size: 70\n",
      "Embedding file number: 130000; dim:50\n",
      "Alphabet size: 1807; direct_match:1174; lower_match:569; OOV_num:63; OOV:0.0348644161594\n",
      "F_ratio: 0\n",
      "Instance size: 1059\n",
      "F_ratio: 0\n",
      "Instance size: 2589\n",
      "F_ratio: 0\n",
      "Instance size: 1132\n",
      "Train instance:oracle:first = 1059 : Nbest over large than data candidate num, 11 > 10\n",
      "0.894409937888 : 0.797507788162\n",
      "Dev instance:oracle:first = 2589 : 0.980044345898 : 0.925139664804\n",
      "Test instance:oracle:first = 1132 : 0.904276985743 : 0.762886597938\n",
      "Start to train model in normal step......\n",
      "Train on 1059 samples, validate on 2589 samples\n",
      "Epoch 1/1\n",
      "1059/1059 [==============================] - 16s 16ms/step - loss: 0.1377 - acc: 0.3371 - val_loss: 0.1273 - val_acc: 0.3758\n",
      "\n",
      "Epoch 00001: saving model to ./results/MODEL.B_CHAR.True_DB.True_Mask.True_BILSTM.True_Emb.SENNA.emb_drop.0.2_GD.adam_Iter.01.hdf5\n",
      "Start to load existing model ./results/MODEL.B_CHAR.True_DB.True_Mask.True_BILSTM.True_Emb.SENNA.emb_drop.0.2_GD.adam_Iter.......\n",
      "Result has been written in ./results/MODEL.B_CHAR.True_DB.True_Mask.True_BILSTM.True_Emb.SENNA.emb_drop.0.2_GD.adam_Iter.origindev\n",
      "Sentence num: 261\n",
      "Result has been written in ./results/MODEL.B_CHAR.True_DB.True_Mask.True_BILSTM.True_Emb.SENNA.emb_drop.0.2_GD.adam_Iter.origindev\n",
      "Sentence num: 261\n",
      "Result has been written in ./results/MODEL.B_CHAR.True_DB.True_Mask.True_BILSTM.True_Emb.SENNA.emb_drop.0.2_GD.adam_Iter.origindev\n",
      "Sentence num: 261\n",
      "Result has been written in ./results/MODEL.B_CHAR.True_DB.True_Mask.True_BILSTM.True_Emb.SENNA.emb_drop.0.2_GD.adam_Iter.origindev\n",
      "Sentence num: 261\n",
      "Result has been written in ./results/MODEL.B_CHAR.True_DB.True_Mask.True_BILSTM.True_Emb.SENNA.emb_drop.0.2_GD.adam_Iter.origindev\n",
      "Sentence num: 261\n",
      "epoch: 0; alpha:0.7; best f: 0.933930571109; choose accuracy:0.900383141762\n",
      "Best epoch/alpha: 0/0.7; P/R/F: 0.939189189189/0.928730512249/0.933930571109;  choose accuracy: 0.900383141762\n",
      "Loading model: ./results/MODEL.B_CHAR.True_DB.True_Mask.True_BILSTM.True_Emb.SENNA.emb_drop.0.2_GD.adam_Iter.00.hdf5\n",
      "Test data: P: 0.779661016949 , R: 0.738955823293, F: 0.758762886598, Accuracy: 0.657894736842\n",
      "Result has been written in ./results/MODEL.B_CHAR.True_DB.True_Mask.True_BILSTM.True_Emb.SENNA.emb_drop.0.2_GD.adam_Iter.00.test\n",
      "Sentence num: 114\n",
      "Result has been written in ./results/MODEL.B_CHAR.True_DB.True_Mask.True_BILSTM.True_Emb.SENNA.emb_drop.0.2_GD.adam_Iter.00.dev\n",
      "Sentence num: 261\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
