{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAM-CNN Demo\n",
    "\n",
    "### First import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import optparse\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "from src.utils import create_input\n",
    "import src.loader as loader\n",
    "\n",
    "from src.utils import evaluate, eval_script, eval_temp, save_mappings, reload_mappings\n",
    "from src.loader import word_mapping, char_mapping, tag_mapping, pt_mapping\n",
    "from src.loader import update_tag_scheme, prepare_dataset\n",
    "from src.loader import augment_with_pretrained\n",
    "from gensim.models import word2vec\n",
    "from src.GRAMCNN import GRAMCNN\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "models_path = \"./models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the class of the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramCnnNet:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.train_path = \"./dataset/CHEM/train.tsv\"\n",
    "        self.dev = \"./dataset/CHEM/dev.tsv\"\n",
    "        self.test = \"./dataset/CHEM/test.tsv\"\n",
    "        self.pre_emb = \"./dataset/vectorfile/PubMed-shuffle-win-30.bin\"\n",
    "\n",
    "        self.dropout = 0.5\n",
    "        self.word_lstm_dim = 675\n",
    "        self.word_dim = 200\n",
    "        self.hidden_layer = 7\n",
    "        self.kernel_size = \"2,3,4\"\n",
    "        self.kernel_num = \"40,40,40\"\n",
    "        self.padding = 1\n",
    "        self.pts = 0\n",
    "        self.tag_scheme = \"iob\"\n",
    "\n",
    "        self.lower = 0\n",
    "        self.zeros = 0\n",
    "        self.char_dim = 1\n",
    "        self.char_lstm_dim = 1\n",
    "        self.char_bidirect = 1\n",
    "        self.word_bidirect = 1\n",
    "        self.all_emb = 1\n",
    "        self.cap_dim = 0\n",
    "        self.crf = 1\n",
    "        self.lr_method = \"sgd-lr_.005\"\n",
    "        self.use_word = 1\n",
    "        self.use_char = 1\n",
    "        self.reloaded = 1\n",
    "        \n",
    "        # Parse parameters\n",
    "        parameters = OrderedDict()\n",
    "        #IOB OR IOEB\n",
    "        parameters['padding'] = self.padding == 1\n",
    "        parameters['tag_scheme'] = self.tag_scheme\n",
    "        parameters['lower'] = self.lower == 1\n",
    "        parameters['zeros'] = self.zeros == 1\n",
    "        parameters['char_dim'] = self.char_dim\n",
    "        parameters['char_lstm_dim'] = self.char_lstm_dim\n",
    "        parameters['char_bidirect'] = self.char_bidirect == 1\n",
    "        parameters['word_dim'] = self.word_dim\n",
    "        parameters['word_lstm_dim'] = self.word_lstm_dim\n",
    "        parameters['word_bidirect'] = self.word_bidirect == 1\n",
    "        parameters['pre_emb'] = self.pre_emb\n",
    "        parameters['all_emb'] = self.all_emb == 1\n",
    "        parameters['cap_dim'] = self.cap_dim\n",
    "        parameters['crf'] = self.crf == 1\n",
    "        parameters['dropout'] = self.dropout\n",
    "        parameters['lr_method'] = self.lr_method\n",
    "        parameters['use_word'] = self.use_word == 1\n",
    "        parameters['use_char'] = self.use_char == 1\n",
    "        parameters['hidden_layer'] = self.hidden_layer\n",
    "        parameters['reload'] = self.reloaded == 1\n",
    "        parameters['kernels'] = [2,3,4,5] if type(self.kernel_size) == str else map(lambda x : int(x), self.kernel_size)\n",
    "        parameters['num_kernels'] = [100,100,100,100] if type(self.kernel_num) == str else map(lambda x : int(x), self.kernel_num)\n",
    "        parameters['pts'] = self.pts == 1\n",
    "        \n",
    "        self.parameters = parameters\n",
    "        self.model_name = \"chemner_2\"\n",
    "        \n",
    "    def convert_ground_truth(self, data, *args, **kwargs):  # <--- implemented PER class\n",
    "        pass\n",
    "\n",
    "    def read_dataset(self, file_dict, dataset_name):  # <--- implemented PER class\n",
    "        \n",
    "        self.train_path = file_dict + dataset_name[0]\n",
    "        self.dev = file_dict + dataset_name[1]\n",
    "        self.test = file_dict + dataset_name[2]\n",
    "        \n",
    "        \n",
    "        if 'bin' in self.parameters['pre_emb']:\n",
    "            self.wordmodel = word2vec.Word2Vec.load_word2vec_format(self.parameters['pre_emb'], binary=True)\n",
    "        else:\n",
    "            self.wordmodel = word2vec.Word2Vec.load_word2vec_format(self.parameters['pre_emb'], binary=False)\n",
    "            \n",
    "        # Data parameters\n",
    "        lower = self.parameters['lower']\n",
    "        zeros = self.parameters['zeros']\n",
    "        tag_scheme = self.parameters['tag_scheme']\n",
    "\n",
    "        # Load sentences\n",
    "        train_sentences = loader.load_sentences(self.train_path, self.lower, self.zeros)\n",
    "        dev_sentences = loader.load_sentences(self.dev, self.lower, self.zeros)\n",
    "\n",
    "        avg_len = sum([len(i) for i in train_sentences]) / float(len(train_sentences))\n",
    "        print \"train avg len: %d\" % (avg_len)\n",
    "\n",
    "        if os.path.isfile(self.test):\n",
    "            test_sentences = loader.load_sentences(self.test, self.lower, self.zeros)\n",
    "        \n",
    "        \n",
    "        # Sample\n",
    "        train_sentences = train_sentences[:200]\n",
    "        dev_sentences = dev_sentences[:200]\n",
    "        test_sentences = test_sentences[:50]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Use selected tagging scheme (IOB / IOBES)\n",
    "        update_tag_scheme(train_sentences, self.tag_scheme)\n",
    "        update_tag_scheme(dev_sentences, self.tag_scheme)\n",
    "        if os.path.isfile(self.test):\n",
    "            update_tag_scheme(test_sentences, self.tag_scheme)\n",
    "\n",
    "        dt_sentences = []\n",
    "        if os.path.isfile(self.test):\n",
    "            dt_sentences = dev_sentences + test_sentences\n",
    "        else:\n",
    "            dt_sentences = dev_sentences\n",
    "            \n",
    "        # Create a dictionary / mapping of words\n",
    "        # If we use pretrained embeddings, we add them to the dictionary.\n",
    "        self.word_to_id = []\n",
    "        self.char_to_id = []\n",
    "        self.pt_to_id = []\n",
    "        self.tag_to_id = []\n",
    "        \n",
    "        if not self.parameters['reload']:\n",
    "            if self.parameters['pre_emb']:\n",
    "                # mapping of words frenquency decreasing\n",
    "                self.dico_words_train = word_mapping(train_sentences, self.lower)[0]\n",
    "                self.dico_words, self.word_to_id, self.id_to_word = augment_with_pretrained(\n",
    "                    self.dico_words_train.copy(),\n",
    "                    self.wordmodel,\n",
    "                    list(itertools.chain.from_iterable(\n",
    "                        [[w[0] for w in s] for s in dt_sentences])\n",
    "                    ) if not self.parameters['all_emb'] else None\n",
    "                )\n",
    "            else:\n",
    "                self.dico_words, self.word_to_id, self.id_to_word = word_mapping(train_sentences, self.lower)\n",
    "                self.dico_words_train = dico_words\n",
    "\n",
    "\n",
    "            # Create a dictionary and a mapping for words / POS tags / tags\n",
    "            self.dico_chars, self.char_to_id, self.id_to_char = char_mapping(train_sentences)\n",
    "            self.dico_tags, self.tag_to_id, self.id_to_tag = tag_mapping(train_sentences)\n",
    "            self.dico_pts, self.pt_to_id, self.id_to_pt = pt_mapping(train_sentences + dev_sentences)\n",
    "            if not os.path.exists(os.path.join(models_path, self.model_name)):\n",
    "                    os.makedirs(os.path.join(models_path,self.model_name))\n",
    "            save_mappings(os.path.join(models_path, self.model_name, 'mappings.pkl'), self.word_to_id, self.char_to_id, self.tag_to_id, self.pt_to_id, self.dico_words, self.id_to_tag)\n",
    "        else:\n",
    "            self.word_to_id, self.char_to_id, self.tag_to_id, self.pt_to_id, self.dico_words, self.id_to_tag = reload_mappings(os.path.join(models_path, self.model_name, 'mappings.pkl'))\n",
    "            self.dico_words_train = self.dico_words\n",
    "            self.id_to_word = {v: k for k, v in self.word_to_id.items()}\n",
    "            \n",
    "        # Index data\n",
    "        m3 = 0\n",
    "        train_data,m1 = prepare_dataset(\n",
    "            train_sentences, self.word_to_id, self.char_to_id, self.tag_to_id, self.pt_to_id, self.lower\n",
    "        )\n",
    "        dev_data,m2 = prepare_dataset(\n",
    "            dev_sentences, self.word_to_id, self.char_to_id, self.tag_to_id, self.pt_to_id, self.lower\n",
    "        )\n",
    "        if os.path.isfile(self.test):\n",
    "            test_data,m3 = prepare_dataset(\n",
    "                test_sentences, self.word_to_id, self.char_to_id, self.tag_to_id, self.pt_to_id, self.lower\n",
    "            )\n",
    "\n",
    "        self.max_seq_len = max(m1,m2,m3)\n",
    "        print \"max length is %i\" % (self.max_seq_len)\n",
    "\n",
    "        print \"%i / %i  sentences in train / dev.\" % (\n",
    "           len(train_data), len(dev_data))\n",
    "        \n",
    "        return (train_data, dev_data, test_data, test_sentences)\n",
    "            \n",
    "            \n",
    "    def train(self, data):  # <--- implemented PER class\n",
    "        #\n",
    "        # Train network\n",
    "        #\n",
    "        train_data, dev_data, test_data, test_sentences = data\n",
    "        \n",
    "        singletons = set([self.word_to_id[k] for k, v\n",
    "                          in self.dico_words_train.items() if v == 1])\n",
    "\n",
    "        n_epochs = 2  # number of epochs over the training set\n",
    "        freq_eval = 2000  # evaluate on dev every freq_eval steps\n",
    "        best_dev = -np.inf\n",
    "        best_test = -np.inf\n",
    "        count = 0\n",
    "        \n",
    "        #initilaze the embedding matrix\n",
    "        word_emb_weight = np.zeros((len(self.dico_words), self.parameters['word_dim']))\n",
    "        c_found = 0\n",
    "        c_lower = 0\n",
    "        c_zeros = 0\n",
    "        n_words = len(self.dico_words)\n",
    "        for i in xrange(n_words):\n",
    "                word = self.id_to_word[i]\n",
    "                if word in self.wordmodel:\n",
    "                    word_emb_weight[i] = self.wordmodel[word]\n",
    "                    c_found += 1\n",
    "                elif re.sub('\\d', '0', word) in self.wordmodel:\n",
    "                    word_emb_weight[i] = self.wordmodel[\n",
    "                        re.sub('\\d', '0', word)\n",
    "                    ]\n",
    "                    c_zeros += 1\n",
    "\n",
    "        print 'Loaded %i pretrained embeddings.' % len(self.wordmodel.vocab)\n",
    "        print ('%i / %i (%.4f%%) words have been initialized with '\n",
    "               'pretrained embeddings.') % (\n",
    "                    c_found + c_lower + c_zeros, n_words,\n",
    "                    100. * (c_found + c_lower + c_zeros) / n_words\n",
    "              )\n",
    "        print ('%i found directly, %i after lowercasing, '\n",
    "               '%i after lowercasing + zero.') % (\n",
    "                  c_found, c_lower, c_zeros\n",
    "              )\n",
    "        \n",
    "        self.gramcnn = GRAMCNN(n_words, len(self.char_to_id), len(self.pt_to_id),\n",
    "                    use_word = self.parameters['use_word'],\n",
    "                    use_char = self.parameters['use_char'],\n",
    "                    use_pts = self.parameters['pts'],\n",
    "                    num_classes = len(self.id_to_tag),\n",
    "                    word_emb = self.parameters['word_dim'],\n",
    "                    drop_out = self.parameters['dropout'],\n",
    "                    word2vec = word_emb_weight, feature_maps=self.parameters['num_kernels'],#,200,200, 200,200],\n",
    "                    kernels = self.parameters['kernels'], hidden_size = self.parameters['word_lstm_dim'], hidden_layers = self.parameters['hidden_layer'],\n",
    "                    padding = self.parameters['padding'], max_seq_len = self.max_seq_len, train_size = len(train_data))\n",
    "\n",
    "        for epoch in xrange(n_epochs):\n",
    "            epoch_costs = []\n",
    "            print \"Starting epoch %i...\" % epoch\n",
    "            for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "                inputs, word_len = create_input(train_data[index], self.parameters, True, singletons,\n",
    "                    padding = self.parameters['padding'], max_seq_len = self.max_seq_len, use_pts = self.parameters['pts'] )\n",
    "\n",
    "                assert inputs['char_for']\n",
    "                assert inputs['word']\n",
    "                assert inputs['label']\n",
    "\n",
    "                # break\n",
    "                if len(inputs['label']) == 1:\n",
    "                    continue\n",
    "                train_loss = []\n",
    "                temp = []\n",
    "                temp.append(word_len)\n",
    "                batch_loss = self.gramcnn.train(inputs, temp)\n",
    "\n",
    "                train_loss.append(batch_loss)\n",
    "                \n",
    "                if(i % 10 == 0 and i != 0):\n",
    "                    print( \"Epoch[%d], \"%(epoch) + \"Iter \" + str(i))\n",
    "                    \n",
    "                if(i % 500 == 0 and i != 0):\n",
    "                    print( \"Epoch[%d], \"%(epoch) + \"Iter \" + str(i) + \\\n",
    "                            \", Minibatch Loss= \" + \"{:.6f}\".format(np.mean(train_loss[-500:])))\n",
    "                    train_loss = []\n",
    "                \n",
    "                '''\n",
    "                if i % 2000 == 0 and i != 0:\n",
    "                    dev_score = evaluate(parameters, gramcnn, dev_sentences,\n",
    "                                         dev_data, id_to_tag, padding = parameters['padding'],\n",
    "                                         max_seq_len = max_seq_len, use_pts = parameters['pts'])\n",
    "                    print \"dev_score_end\"\n",
    "                    print \"Score on dev: %.5f\" % dev_score\n",
    "                    if dev_score > best_dev:\n",
    "                        best_dev = dev_score\n",
    "                        print \"New best score on dev.\"\n",
    "                        print \"Saving model to disk...\"\n",
    "                        gramcnn.save(models_path ,self.model_name)\n",
    "                    if os.path.isfile(opts.test):\n",
    "                        if i % 8000 == 0 and i != 0:\n",
    "                            test_score = evaluate(parameters, gramcnn, test_sentences,\n",
    "                                                  test_data, id_to_tag, padding = parameters['padding'],\n",
    "                                                  max_seq_len = max_seq_len, use_pts = parameters['pts'])\n",
    "                            print \"Score on test: %.5f\" % test_score\n",
    "                            if test_score > best_test:\n",
    "                                best_test = test_score\n",
    "                                print \"New best score on test.\"\n",
    "                '''\n",
    "        \n",
    "\n",
    "    def predict(self, data, *args, **kwargs):  # <--- implemented PER class WITH requirement on OUTPUT format!\n",
    "        train_data, dev_data, test_data, test_sentences = data\n",
    "        \n",
    "        singletons = set([self.word_to_id[k] for k, v\n",
    "                          in self.dico_words_train.items() if v == 1])\n",
    "\n",
    "        n_epochs = 1000  # number of epochs over the training set\n",
    "        freq_eval = 2000  # evaluate on dev every freq_eval steps\n",
    "        best_dev = -np.inf\n",
    "        best_test = -np.inf\n",
    "        count = 0\n",
    "        \n",
    "        #initilaze the embedding matrix\n",
    "        word_emb_weight = np.zeros((len(self.dico_words), self.parameters['word_dim']))\n",
    "        c_found = 0\n",
    "        c_lower = 0\n",
    "        c_zeros = 0\n",
    "        n_words = len(self.dico_words)\n",
    "        for i in xrange(n_words):\n",
    "                word = self.id_to_word[i]\n",
    "                if word in self.wordmodel:\n",
    "                    word_emb_weight[i] = self.wordmodel[word]\n",
    "                    c_found += 1\n",
    "                elif re.sub('\\d', '0', word) in self.wordmodel:\n",
    "                    word_emb_weight[i] = self.wordmodel[\n",
    "                        re.sub('\\d', '0', word)\n",
    "                    ]\n",
    "                    c_zeros += 1\n",
    "\n",
    "        print 'Loaded %i pretrained embeddings.' % len(self.wordmodel.vocab)\n",
    "        print ('%i / %i (%.4f%%) words have been initialized with '\n",
    "               'pretrained embeddings.') % (\n",
    "                    c_found + c_lower + c_zeros, n_words,\n",
    "                    100. * (c_found + c_lower + c_zeros) / n_words\n",
    "              )\n",
    "        print ('%i found directly, %i after lowercasing, '\n",
    "               '%i after lowercasing + zero.') % (\n",
    "                  c_found, c_lower, c_zeros\n",
    "              )\n",
    "        \n",
    "        test_score, output_path = evaluate(self.parameters, self.gramcnn, test_sentences, test_data, self.id_to_tag, remove = False, max_seq_len = self.max_seq_len, padding = self.parameters['padding'], use_pts = self.parameters['pts'])\n",
    "        \n",
    "        return output_path\n",
    "\n",
    "    def evaluate(self, predictions, groundTruths, *args,\n",
    "                 **kwargs):  # <--- common ACROSS ALL classes. Requirement that INPUT format uses output from predict()!\n",
    "        \n",
    "        train_data, dev_data, test_data, test_sentences = groundTruths\n",
    "        \n",
    "        singletons = set([self.word_to_id[k] for k, v\n",
    "                          in self.dico_words_train.items() if v == 1])\n",
    "\n",
    "        n_epochs = 1000  # number of epochs over the training set\n",
    "        freq_eval = 2000  # evaluate on dev every freq_eval steps\n",
    "        best_dev = -np.inf\n",
    "        best_test = -np.inf\n",
    "        count = 0\n",
    "        \n",
    "        #initilaze the embedding matrix\n",
    "        word_emb_weight = np.zeros((len(self.dico_words), self.parameters['word_dim']))\n",
    "        c_found = 0\n",
    "        c_lower = 0\n",
    "        c_zeros = 0\n",
    "        n_words = len(self.dico_words)\n",
    "        for i in xrange(n_words):\n",
    "                word = self.id_to_word[i]\n",
    "                if word in self.wordmodel:\n",
    "                    word_emb_weight[i] = self.wordmodel[word]\n",
    "                    c_found += 1\n",
    "                elif re.sub('\\d', '0', word) in self.wordmodel:\n",
    "                    word_emb_weight[i] = self.wordmodel[\n",
    "                        re.sub('\\d', '0', word)\n",
    "                    ]\n",
    "                    c_zeros += 1\n",
    "\n",
    "        print 'Loaded %i pretrained embeddings.' % len(self.wordmodel.vocab)\n",
    "        print ('%i / %i (%.4f%%) words have been initialized with '\n",
    "               'pretrained embeddings.') % (\n",
    "                    c_found + c_lower + c_zeros, n_words,\n",
    "                    100. * (c_found + c_lower + c_zeros) / n_words\n",
    "              )\n",
    "        print ('%i found directly, %i after lowercasing, '\n",
    "               '%i after lowercasing + zero.') % (\n",
    "                  c_found, c_lower, c_zeros\n",
    "              )\n",
    "        \n",
    "        test_score, output_path = evaluate(self.parameters, self.gramcnn, test_sentences, test_data, self.id_to_tag, remove = False, max_seq_len = self.max_seq_len, padding = self.parameters['padding'], use_pts = self.parameters['pts'])\n",
    "        \n",
    "        return test_score\n",
    "\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        self.gramcnn.save(models_path ,self.model_name)\n",
    "\n",
    "    \n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \n",
    "        n_epochs = 100  # number of epochs over the training set\n",
    "        freq_eval = 2000  # evaluate on dev every freq_eval steps\n",
    "        best_dev = -np.inf\n",
    "        best_test = -np.inf\n",
    "        count = 0\n",
    "        #self.max_seq_len = m3 if m3 > 200 else 200\n",
    "\n",
    "        #initilaze the embedding matrix\n",
    "        word_emb_weight = np.zeros((len(self.dico_words), self.parameters['word_dim']))\n",
    "        n_words = len(self.dico_words)\n",
    "\n",
    "\n",
    "\n",
    "        self.gramcnn = GRAMCNN(n_words, len(self.char_to_id), len(self.pt_to_id),\n",
    "                            use_word = self.parameters['use_word'],\n",
    "                            use_char = self.parameters['use_char'],\n",
    "                            use_pts = self.parameters['pts'],\n",
    "                            num_classes = len(self.tag_to_id),\n",
    "                            word_emb = self.parameters['word_dim'],\n",
    "                            drop_out = 0,\n",
    "                            word2vec = word_emb_weight,feature_maps=self.parameters['num_kernels'],#,200,200, 200,200],\n",
    "                            kernels=self.parameters['kernels'], hidden_size = self.parameters['word_lstm_dim'], hidden_layers = self.parameters['hidden_layer'],\n",
    "                            padding = self.parameters['padding'], max_seq_len = self.max_seq_len)\n",
    "\n",
    "        \n",
    "        self.gramcnn.load(models_path ,self.model_name)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initail an instance of GramCnnNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GramCnnNet_instance = GramCnnNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataset path and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = \"./dataset/CHEMDNER/\"\n",
    "dataset_name = [\"train.tsv\", \"dev.tsv\", \"test.tsv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data calling the read_dataset method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train avg len: 207\n",
      "max length is 450\n",
      "200 / 200  sentences in train / dev.\n"
     ]
    }
   ],
   "source": [
    "read_data = GramCnnNet_instance.read_dataset(file_dict, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output information shows the average length of trainning data\n",
    "\n",
    "In this demo, we only consider 200 sentences in trainning and development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Effects', u'O'],\n",
       " [u'of', u'O'],\n",
       " [u'docosahexaenoic', u'B-SYSTEMATIC'],\n",
       " [u'acid', u'I-SYSTEMATIC'],\n",
       " [u'and', u'O']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_data[-1][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the preview of our dataset\n",
    "\n",
    "### Now, let's call the train function to train the model\n",
    "\n",
    "As for a demo, we only train for 2 epoch to verify the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2231686 pretrained embeddings.\n",
      "2231688 / 2231844 (99.9930%) words have been initialized with pretrained embeddings.\n",
      "2231686 found directly, 0 after lowercasing, 2 after lowercasing + zero.\n",
      "Starting epoch 0...\n",
      "Epoch[0], Iter 10\n",
      "Epoch[0], Iter 20\n",
      "Epoch[0], Iter 30\n",
      "Epoch[0], Iter 40\n",
      "Epoch[0], Iter 50\n",
      "Epoch[0], Iter 60\n",
      "Epoch[0], Iter 70\n",
      "Epoch[0], Iter 80\n",
      "Epoch[0], Iter 90\n",
      "Epoch[0], Iter 100\n",
      "Epoch[0], Iter 110\n",
      "Epoch[0], Iter 120\n",
      "Epoch[0], Iter 130\n",
      "Epoch[0], Iter 140\n",
      "Epoch[0], Iter 150\n",
      "Epoch[0], Iter 160\n",
      "Epoch[0], Iter 170\n",
      "Epoch[0], Iter 180\n",
      "Epoch[0], Iter 190\n",
      "Starting epoch 1...\n",
      "Epoch[1], Iter 10\n",
      "Epoch[1], Iter 20\n",
      "Epoch[1], Iter 30\n",
      "Epoch[1], Iter 40\n",
      "Epoch[1], Iter 50\n",
      "Epoch[1], Iter 60\n",
      "Epoch[1], Iter 70\n",
      "Epoch[1], Iter 80\n",
      "Epoch[1], Iter 90\n",
      "Epoch[1], Iter 100\n",
      "Epoch[1], Iter 110\n",
      "Epoch[1], Iter 120\n",
      "Epoch[1], Iter 130\n",
      "Epoch[1], Iter 140\n",
      "Epoch[1], Iter 150\n",
      "Epoch[1], Iter 160\n",
      "Epoch[1], Iter 170\n",
      "Epoch[1], Iter 180\n",
      "Epoch[1], Iter 190\n"
     ]
    }
   ],
   "source": [
    "GramCnnNet_instance.train(read_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training, call the predict function to generate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2231686 pretrained embeddings.\n",
      "2231688 / 2231844 (99.9930%) words have been initialized with pretrained embeddings.\n",
      "2231686 found directly, 0 after lowercasing, 2 after lowercasing + zero.\n",
      "Preparing Data\n",
      "run CONLL script\n",
      "Result created\n",
      "processed 9998 tokens with 475 phrases; found: 333 phrases; correct: 176.\n",
      "accuracy:  95.11%; precision:  52.85%; recall:  37.05%; FB1:  43.56\n",
      "     ABBREVIATION: precision:  92.00%; recall:  41.07%; FB1:  56.79  25\n",
      "           FAMILY: precision:  50.00%; recall:   1.72%; FB1:   3.33  2\n",
      "          FORMULA: precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
      "       IDENTIFIER: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "         MULTIPLE: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "       SYSTEMATIC: precision:  34.12%; recall:  26.85%; FB1:  30.05  85\n",
      "          TRIVIAL: precision:  55.91%; recall:  62.12%; FB1:  58.85  220\n",
      "ID     NE  Total      OI-SYSTEMATICB-TRIVIALB-SYSTEMATICB-ABBREVIATIONB-FAMILYB-FORMULAI-TRIVIALI-FAMILYI-FORMULAB-IDENTIFIERI-MULTIPLEI-IDENTIFIERI-ABBREVIATIONB-MULTIPLEB-CLASS  Percent\n",
      " 0      O   9171   9113     36     15      7      0      0      0      0      0      0      0      0      0      0      0      0   99.368\n",
      " 1I-SYSTEMATIC    213     21    181      5      4      0      0      0      2      0      0      0      0      0      0      0      0   84.977\n",
      " 2B-TRIVIAL    198     38     13    128     15      0      0      0      4      0      0      0      0      0      0      0      0   64.646\n",
      " 3B-SYSTEMATIC    108     34     15     18     41      0      0      0      0      0      0      0      0      0      0      0      0   37.963\n",
      " 4B-ABBREVIATION     56     27      3      2      1     23      0      0      0      0      0      0      0      0      0      0      0   41.071\n",
      " 5B-FAMILY     58     19      1     30      7      0      0      0      0      1      0      0      0      0      0      0      0    0.000\n",
      " 6B-FORMULA     40     28      5      0      6      1      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      " 7I-TRIVIAL     67      9     19     17      1      1      0      0     20      0      0      0      0      0      0      0      0   29.851\n",
      " 8I-FAMILY     29      3     12      1      2      0      0      0      9      2      0      0      0      0      0      0      0    6.897\n",
      " 9I-FORMULA     34     21     11      0      1      0      0      0      0      0      1      0      0      0      0      0      0    2.941\n",
      "10B-IDENTIFIER     13     13      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "11I-MULTIPLE      6      2      0      2      0      0      0      0      2      0      0      0      0      0      0      0      0    0.000\n",
      "12I-IDENTIFIER      2      2      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "13I-ABBREVIATION      1      0      0      0      0      0      0      0      1      0      0      0      0      0      0      0      0    0.000\n",
      "14B-MULTIPLE      2      0      0      2      0      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "15B-CLASS      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "9509/9998 (95.10902%)\n",
      "43.56\n",
      "Output file has been created at: ./evaluation/temp_result/eval.1601271.output\n"
     ]
    }
   ],
   "source": [
    "output_file = GramCnnNet_instance.predict(read_data)\n",
    "print \"Output file has been created at: {}\".format(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview output\n",
    "\n",
    "Let's take the file path from the previous output\n",
    "\n",
    "Simply read the output file and display a few line of prediction\n",
    "\n",
    "For each line in file, the first column is a word, second is groundtruth of entity, third is the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(output_file, \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fish O O\\n',\n",
       " 'contains O O\\n',\n",
       " 'both O O\\n',\n",
       " 'beneficial O O\\n',\n",
       " 'substances O O\\n',\n",
       " 'e O O\\n',\n",
       " 'g O O\\n',\n",
       " 'docosahexaenoic B-FAMILY B-TRIVIAL\\n',\n",
       " 'acids I-FAMILY I-TRIVIAL\\n',\n",
       " 'but O O\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[25:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation, and output the F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = GramCnnNet_instance.evaluate(None, read_data)\n",
    "print(\"f1: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the F1 score from the last line of the evaluation output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
