{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConEc (Context Encoders), an extension of word2vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is a Python3 implementation of the following paper - \n",
    "Franziska Horn, Context encoders as a simple but powerful extension of word2vec, Proceedings of the 2nd Workshop on Representation Learning for NLP, Association for Computational Liguistics 2017, Pages 10-14"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This implementation mitigates the shortcomings of word2vec in that word2vec learns only a single representation (embedding) for a word, but it is possible to have a word which means different things in different context..\n",
    "For Ex. Washington (US State) vs Washington (Former President)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This code can be used to train, evaluate an use Context Encoders (ConEc), a powerful context-aware extension of word2vec. This can be used to generate text embeddings for a word with multiple meanings or for Out-Of-Vocabulary (OOV) words by using a trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code implements the following procedure -\n",
    "- Use CBOW word2vec model with negative sampling objective\n",
    "- Train the model on \"text8\" , \"OneBilCorpus\" or \"conll2003\" datasets\n",
    "- Multiply the trained word2vec embeddings with the word's average context vectors (CVs)\n",
    "- A word has global CV and local CV\n",
    "- Choice of alpha in the equation mentioned in the paper determines the emphasis on the word's local context\n",
    "\n",
    "Download the conec folder from this repository and import the conec class into your script\n",
    "\n",
    "Follow instructions given in README.md for more details about the Pre-requisites to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conec import conec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a conec class object, context2vec\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conec object created ... \n",
      "Use this object to get text embeddings or text similarity...\n"
     ]
    }
   ],
   "source": [
    "context2vec = conec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Loading Dataset\n",
    "---\n",
    "Ensure text8/OneBillionCorpus/CoNLL 2003 data is downloaded and present in /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ....\n",
      "Dataset loaded into conec object ....\n"
     ]
    }
   ],
   "source": [
    "context2vec.read_Dataset('text8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training \n",
    "Train the conec object on 5 iterations of word2vec model with negative modeling, \n",
    "followed by introducing the context vectors to this pre-trained model\n",
    "Modify the iterations, modelType (\"cbow\" or \"sg\") and embedding dimesion (embed_dim) as required\n",
    "Here, we are only training the model for 1 iteration to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-30 02:13:10,380 : INFO : collecting all words and their counts\n",
      "2019-04-30 02:13:10,386 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 unique words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conec object being trained using text8 dataset present in the directory data/text8 ....\n",
      "Creating word2vec model ....\n",
      "<conec.Text8Corpus object at 0x7f3b689a2358>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-30 02:13:19,176 : INFO : PROGRESS: at sentence #10000, processed 10000000 words and 189074 unique words\n",
      "2019-04-30 02:13:26,616 : INFO : collected 253854 unique words from a corpus of 17005207 words and 17006 sentences\n",
      "2019-04-30 02:13:26,776 : INFO : total of 71290 unique words after removing those with count < 5\n",
      "2019-04-30 02:13:26,814 : INFO : constructing a table with noise distribution from 71290 words\n",
      "2019-04-30 02:14:38,368 : INFO : training model on 71290 vocabulary and 200 features\n",
      "2019-04-30 02:14:58,460 : INFO : PROGRESS: at 0.38% words, alpha 0.00500, 3203 words/s\n",
      "2019-04-30 02:15:18,584 : INFO : PROGRESS: at 0.71% words, alpha 0.00500, 2970 words/s\n",
      "2019-04-30 02:15:38,802 : INFO : PROGRESS: at 1.10% words, alpha 0.00500, 3054 words/s\n",
      "2019-04-30 02:15:59,224 : INFO : PROGRESS: at 1.42% words, alpha 0.00500, 2943 words/s\n",
      "2019-04-30 02:16:19,547 : INFO : PROGRESS: at 1.72% words, alpha 0.00500, 2848 words/s\n",
      "2019-04-30 02:16:39,694 : INFO : PROGRESS: at 2.04% words, alpha 0.00500, 2805 words/s\n",
      "2019-04-30 02:17:00,092 : INFO : PROGRESS: at 2.38% words, alpha 0.00500, 2803 words/s\n",
      "2019-04-30 02:17:20,185 : INFO : PROGRESS: at 2.70% words, alpha 0.00500, 2790 words/s\n",
      "2019-04-30 02:17:40,193 : INFO : PROGRESS: at 3.04% words, alpha 0.00500, 2797 words/s\n",
      "2019-04-30 02:18:00,317 : INFO : PROGRESS: at 3.46% words, alpha 0.00500, 2868 words/s\n",
      "2019-04-30 02:18:20,453 : INFO : PROGRESS: at 3.82% words, alpha 0.00500, 2874 words/s\n",
      "2019-04-30 02:18:40,709 : INFO : PROGRESS: at 4.22% words, alpha 0.00500, 2914 words/s\n",
      "2019-04-30 02:19:00,841 : INFO : PROGRESS: at 4.59% words, alpha 0.00500, 2925 words/s\n",
      "2019-04-30 02:19:21,022 : INFO : PROGRESS: at 5.00% words, alpha 0.00500, 2956 words/s\n",
      "2019-04-30 02:19:41,191 : INFO : PROGRESS: at 5.39% words, alpha 0.00500, 2976 words/s\n",
      "2019-04-30 02:20:01,270 : INFO : PROGRESS: at 5.79% words, alpha 0.00500, 2996 words/s\n",
      "2019-04-30 02:20:21,535 : INFO : PROGRESS: at 6.18% words, alpha 0.00500, 3010 words/s\n",
      "2019-04-30 02:20:41,663 : INFO : PROGRESS: at 6.56% words, alpha 0.00500, 3021 words/s\n",
      "2019-04-30 02:21:01,912 : INFO : PROGRESS: at 6.96% words, alpha 0.00500, 3033 words/s\n",
      "2019-04-30 02:21:22,076 : INFO : PROGRESS: at 7.36% words, alpha 0.00500, 3047 words/s\n",
      "2019-04-30 02:21:42,305 : INFO : PROGRESS: at 7.78% words, alpha 0.00500, 3070 words/s\n",
      "2019-04-30 02:22:02,443 : INFO : PROGRESS: at 8.20% words, alpha 0.00500, 3086 words/s\n",
      "2019-04-30 02:22:22,637 : INFO : PROGRESS: at 8.60% words, alpha 0.00500, 3098 words/s\n",
      "2019-04-30 02:22:42,885 : INFO : PROGRESS: at 9.02% words, alpha 0.00500, 3113 words/s\n",
      "2019-04-30 02:23:03,028 : INFO : PROGRESS: at 9.45% words, alpha 0.00500, 3130 words/s\n",
      "2019-04-30 02:23:23,126 : INFO : PROGRESS: at 9.87% words, alpha 0.00500, 3145 words/s\n",
      "2019-04-30 02:23:43,312 : INFO : PROGRESS: at 10.29% words, alpha 0.00500, 3158 words/s\n",
      "2019-04-30 02:24:03,570 : INFO : PROGRESS: at 10.72% words, alpha 0.00500, 3171 words/s\n",
      "2019-04-30 02:24:23,609 : INFO : PROGRESS: at 11.14% words, alpha 0.00500, 3181 words/s\n",
      "2019-04-30 02:24:43,760 : INFO : PROGRESS: at 11.57% words, alpha 0.00500, 3195 words/s\n",
      "2019-04-30 02:25:03,818 : INFO : PROGRESS: at 12.00% words, alpha 0.00500, 3208 words/s\n",
      "2019-04-30 02:25:24,091 : INFO : PROGRESS: at 12.43% words, alpha 0.00500, 3218 words/s\n",
      "2019-04-30 02:25:44,165 : INFO : PROGRESS: at 12.83% words, alpha 0.00500, 3222 words/s\n",
      "2019-04-30 02:26:04,254 : INFO : PROGRESS: at 13.26% words, alpha 0.00500, 3232 words/s\n",
      "2019-04-30 02:26:24,301 : INFO : PROGRESS: at 13.68% words, alpha 0.00500, 3241 words/s\n",
      "2019-04-30 02:26:44,439 : INFO : PROGRESS: at 14.11% words, alpha 0.00500, 3248 words/s\n",
      "2019-04-30 02:27:04,530 : INFO : PROGRESS: at 14.53% words, alpha 0.00500, 3256 words/s\n",
      "2019-04-30 02:27:24,599 : INFO : PROGRESS: at 14.95% words, alpha 0.00500, 3262 words/s\n",
      "2019-04-30 02:27:44,826 : INFO : PROGRESS: at 15.37% words, alpha 0.00500, 3268 words/s\n",
      "2019-04-30 02:28:04,892 : INFO : PROGRESS: at 15.80% words, alpha 0.00500, 3275 words/s\n",
      "2019-04-30 02:28:25,099 : INFO : PROGRESS: at 16.21% words, alpha 0.00500, 3279 words/s\n",
      "2019-04-30 02:28:45,310 : INFO : PROGRESS: at 16.63% words, alpha 0.00500, 3283 words/s\n",
      "2019-04-30 02:29:05,348 : INFO : PROGRESS: at 17.05% words, alpha 0.00500, 3289 words/s\n",
      "2019-04-30 02:29:25,394 : INFO : PROGRESS: at 17.48% words, alpha 0.00500, 3295 words/s\n",
      "2019-04-30 02:29:45,410 : INFO : PROGRESS: at 17.89% words, alpha 0.00500, 3298 words/s\n",
      "2019-04-30 02:30:05,508 : INFO : PROGRESS: at 18.29% words, alpha 0.00500, 3299 words/s\n",
      "2019-04-30 02:30:25,630 : INFO : PROGRESS: at 18.68% words, alpha 0.00500, 3297 words/s\n",
      "2019-04-30 02:30:45,712 : INFO : PROGRESS: at 19.10% words, alpha 0.00500, 3301 words/s\n",
      "2019-04-30 02:31:05,888 : INFO : PROGRESS: at 19.53% words, alpha 0.00500, 3306 words/s\n",
      "2019-04-30 02:31:25,916 : INFO : PROGRESS: at 19.94% words, alpha 0.00500, 3309 words/s\n",
      "2019-04-30 02:31:46,139 : INFO : PROGRESS: at 20.36% words, alpha 0.00500, 3312 words/s\n",
      "2019-04-30 02:32:06,352 : INFO : PROGRESS: at 20.79% words, alpha 0.00500, 3317 words/s\n",
      "2019-04-30 02:32:26,375 : INFO : PROGRESS: at 21.21% words, alpha 0.00500, 3321 words/s\n",
      "2019-04-30 02:32:46,562 : INFO : PROGRESS: at 21.64% words, alpha 0.00500, 3325 words/s\n",
      "2019-04-30 02:33:06,619 : INFO : PROGRESS: at 22.07% words, alpha 0.00500, 3329 words/s\n",
      "2019-04-30 02:33:26,859 : INFO : PROGRESS: at 22.49% words, alpha 0.00500, 3331 words/s\n",
      "2019-04-30 02:33:47,041 : INFO : PROGRESS: at 22.90% words, alpha 0.00500, 3333 words/s\n",
      "2019-04-30 02:34:07,247 : INFO : PROGRESS: at 23.31% words, alpha 0.00500, 3334 words/s\n",
      "2019-04-30 02:34:27,338 : INFO : PROGRESS: at 23.73% words, alpha 0.00500, 3337 words/s\n",
      "2019-04-30 02:34:47,563 : INFO : PROGRESS: at 24.13% words, alpha 0.00500, 3336 words/s\n",
      "2019-04-30 02:35:07,710 : INFO : PROGRESS: at 24.53% words, alpha 0.00500, 3336 words/s\n",
      "2019-04-30 02:35:27,896 : INFO : PROGRESS: at 24.94% words, alpha 0.00500, 3337 words/s\n",
      "2019-04-30 02:35:47,916 : INFO : PROGRESS: at 25.35% words, alpha 0.00500, 3338 words/s\n",
      "2019-04-30 02:36:08,016 : INFO : PROGRESS: at 25.76% words, alpha 0.00500, 3339 words/s\n",
      "2019-04-30 02:36:28,045 : INFO : PROGRESS: at 26.18% words, alpha 0.00500, 3342 words/s\n",
      "2019-04-30 02:36:48,295 : INFO : PROGRESS: at 26.61% words, alpha 0.00500, 3345 words/s\n",
      "2019-04-30 02:37:08,544 : INFO : PROGRESS: at 27.03% words, alpha 0.00500, 3347 words/s\n",
      "2019-04-30 02:37:28,805 : INFO : PROGRESS: at 27.43% words, alpha 0.00500, 3346 words/s\n",
      "2019-04-30 02:37:49,048 : INFO : PROGRESS: at 27.84% words, alpha 0.00500, 3347 words/s\n",
      "2019-04-30 02:38:09,053 : INFO : PROGRESS: at 28.26% words, alpha 0.00500, 3350 words/s\n",
      "2019-04-30 02:38:29,253 : INFO : PROGRESS: at 28.68% words, alpha 0.00500, 3351 words/s\n",
      "2019-04-30 02:38:49,271 : INFO : PROGRESS: at 28.98% words, alpha 0.00500, 3339 words/s\n",
      "2019-04-30 02:39:09,411 : INFO : PROGRESS: at 29.31% words, alpha 0.00500, 3331 words/s\n",
      "2019-04-30 02:39:29,709 : INFO : PROGRESS: at 29.63% words, alpha 0.00500, 3321 words/s\n",
      "2019-04-30 02:39:49,742 : INFO : PROGRESS: at 30.01% words, alpha 0.00500, 3320 words/s\n",
      "2019-04-30 02:40:10,005 : INFO : PROGRESS: at 30.40% words, alpha 0.00500, 3318 words/s\n",
      "2019-04-30 02:40:30,175 : INFO : PROGRESS: at 30.82% words, alpha 0.00500, 3320 words/s\n",
      "2019-04-30 02:40:50,248 : INFO : PROGRESS: at 31.23% words, alpha 0.00500, 3322 words/s\n",
      "2019-04-30 02:41:10,425 : INFO : PROGRESS: at 31.66% words, alpha 0.00500, 3325 words/s\n",
      "2019-04-30 02:41:30,516 : INFO : PROGRESS: at 32.07% words, alpha 0.00500, 3326 words/s\n",
      "2019-04-30 02:41:50,639 : INFO : PROGRESS: at 32.47% words, alpha 0.00500, 3326 words/s\n",
      "2019-04-30 02:42:10,832 : INFO : PROGRESS: at 32.90% words, alpha 0.00500, 3329 words/s\n",
      "2019-04-30 02:42:31,046 : INFO : PROGRESS: at 33.31% words, alpha 0.00500, 3330 words/s\n",
      "2019-04-30 02:42:51,195 : INFO : PROGRESS: at 33.72% words, alpha 0.00500, 3331 words/s\n",
      "2019-04-30 02:43:11,448 : INFO : PROGRESS: at 34.12% words, alpha 0.00500, 3330 words/s\n",
      "2019-04-30 02:43:31,457 : INFO : PROGRESS: at 34.50% words, alpha 0.00500, 3328 words/s\n",
      "2019-04-30 02:43:51,502 : INFO : PROGRESS: at 34.89% words, alpha 0.00500, 3327 words/s\n",
      "2019-04-30 02:44:11,780 : INFO : PROGRESS: at 35.26% words, alpha 0.00500, 3324 words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-30 02:44:31,911 : INFO : PROGRESS: at 35.64% words, alpha 0.00500, 3322 words/s\n",
      "2019-04-30 02:44:52,089 : INFO : PROGRESS: at 36.06% words, alpha 0.00500, 3324 words/s\n",
      "2019-04-30 02:45:12,135 : INFO : PROGRESS: at 36.49% words, alpha 0.00500, 3327 words/s\n",
      "2019-04-30 02:45:32,373 : INFO : PROGRESS: at 36.91% words, alpha 0.00500, 3328 words/s\n",
      "2019-04-30 02:45:52,438 : INFO : PROGRESS: at 37.28% words, alpha 0.00500, 3326 words/s\n",
      "2019-04-30 02:46:12,642 : INFO : PROGRESS: at 37.67% words, alpha 0.00500, 3325 words/s\n",
      "2019-04-30 02:46:32,792 : INFO : PROGRESS: at 38.04% words, alpha 0.00500, 3322 words/s\n",
      "2019-04-30 02:46:53,034 : INFO : PROGRESS: at 38.43% words, alpha 0.00500, 3321 words/s\n",
      "2019-04-30 02:47:13,074 : INFO : PROGRESS: at 38.81% words, alpha 0.00500, 3319 words/s\n",
      "2019-04-30 02:47:33,182 : INFO : PROGRESS: at 39.22% words, alpha 0.00500, 3320 words/s\n",
      "2019-04-30 02:47:53,314 : INFO : PROGRESS: at 39.62% words, alpha 0.00500, 3321 words/s\n",
      "2019-04-30 02:48:13,337 : INFO : PROGRESS: at 40.03% words, alpha 0.00500, 3322 words/s\n",
      "2019-04-30 02:48:33,403 : INFO : PROGRESS: at 40.43% words, alpha 0.00500, 3321 words/s\n",
      "2019-04-30 02:48:53,416 : INFO : PROGRESS: at 40.84% words, alpha 0.00500, 3322 words/s\n",
      "2019-04-30 02:49:13,651 : INFO : PROGRESS: at 41.26% words, alpha 0.00500, 3324 words/s\n",
      "2019-04-30 02:49:33,653 : INFO : PROGRESS: at 41.67% words, alpha 0.00500, 3325 words/s\n",
      "2019-04-30 02:49:53,896 : INFO : PROGRESS: at 42.08% words, alpha 0.00500, 3325 words/s\n",
      "2019-04-30 02:50:13,978 : INFO : PROGRESS: at 42.50% words, alpha 0.00500, 3327 words/s\n",
      "2019-04-30 02:50:34,021 : INFO : PROGRESS: at 42.91% words, alpha 0.00500, 3328 words/s\n",
      "2019-04-30 02:50:54,296 : INFO : PROGRESS: at 43.33% words, alpha 0.00500, 3329 words/s\n",
      "2019-04-30 02:51:14,503 : INFO : PROGRESS: at 43.74% words, alpha 0.00500, 3330 words/s\n",
      "2019-04-30 02:51:34,843 : INFO : PROGRESS: at 44.17% words, alpha 0.00500, 3332 words/s\n",
      "2019-04-30 02:51:55,015 : INFO : PROGRESS: at 44.58% words, alpha 0.00500, 3333 words/s\n",
      "2019-04-30 02:52:15,023 : INFO : PROGRESS: at 45.00% words, alpha 0.00500, 3334 words/s\n",
      "2019-04-30 02:52:35,174 : INFO : PROGRESS: at 45.41% words, alpha 0.00500, 3334 words/s\n",
      "2019-04-30 02:52:55,342 : INFO : PROGRESS: at 45.82% words, alpha 0.00500, 3335 words/s\n",
      "2019-04-30 02:53:15,410 : INFO : PROGRESS: at 46.23% words, alpha 0.00500, 3336 words/s\n",
      "2019-04-30 02:53:35,622 : INFO : PROGRESS: at 46.64% words, alpha 0.00500, 3336 words/s\n",
      "2019-04-30 02:53:55,836 : INFO : PROGRESS: at 47.04% words, alpha 0.00500, 3336 words/s\n",
      "2019-04-30 02:54:16,090 : INFO : PROGRESS: at 47.47% words, alpha 0.00500, 3338 words/s\n",
      "2019-04-30 02:54:36,175 : INFO : PROGRESS: at 47.91% words, alpha 0.00500, 3341 words/s\n",
      "2019-04-30 02:54:56,325 : INFO : PROGRESS: at 48.35% words, alpha 0.00500, 3343 words/s\n",
      "2019-04-30 02:55:16,379 : INFO : PROGRESS: at 48.79% words, alpha 0.00500, 3346 words/s\n",
      "2019-04-30 02:55:36,643 : INFO : PROGRESS: at 49.21% words, alpha 0.00500, 3347 words/s\n",
      "2019-04-30 02:55:56,846 : INFO : PROGRESS: at 49.65% words, alpha 0.00500, 3349 words/s\n",
      "2019-04-30 02:56:17,113 : INFO : PROGRESS: at 50.08% words, alpha 0.00500, 3351 words/s\n",
      "2019-04-30 02:56:37,185 : INFO : PROGRESS: at 50.48% words, alpha 0.00500, 3351 words/s\n",
      "2019-04-30 02:56:57,413 : INFO : PROGRESS: at 50.90% words, alpha 0.00500, 3352 words/s\n",
      "2019-04-30 02:57:17,418 : INFO : PROGRESS: at 51.32% words, alpha 0.00500, 3353 words/s\n",
      "2019-04-30 02:57:37,549 : INFO : PROGRESS: at 51.74% words, alpha 0.00500, 3354 words/s\n",
      "2019-04-30 02:57:57,772 : INFO : PROGRESS: at 52.16% words, alpha 0.00500, 3355 words/s\n",
      "2019-04-30 02:58:17,957 : INFO : PROGRESS: at 52.57% words, alpha 0.00500, 3355 words/s\n",
      "2019-04-30 02:58:38,069 : INFO : PROGRESS: at 53.01% words, alpha 0.00500, 3357 words/s\n",
      "2019-04-30 02:58:58,083 : INFO : PROGRESS: at 53.44% words, alpha 0.00500, 3359 words/s\n",
      "2019-04-30 02:59:18,167 : INFO : PROGRESS: at 53.86% words, alpha 0.00500, 3360 words/s\n",
      "2019-04-30 02:59:38,210 : INFO : PROGRESS: at 54.26% words, alpha 0.00500, 3360 words/s\n",
      "2019-04-30 02:59:58,330 : INFO : PROGRESS: at 54.69% words, alpha 0.00500, 3361 words/s\n",
      "2019-04-30 03:00:18,413 : INFO : PROGRESS: at 55.10% words, alpha 0.00500, 3362 words/s\n",
      "2019-04-30 03:00:38,624 : INFO : PROGRESS: at 55.52% words, alpha 0.00500, 3363 words/s\n",
      "2019-04-30 03:00:58,720 : INFO : PROGRESS: at 55.94% words, alpha 0.00500, 3364 words/s\n",
      "2019-04-30 03:01:18,738 : INFO : PROGRESS: at 56.37% words, alpha 0.00500, 3365 words/s\n",
      "2019-04-30 03:01:38,820 : INFO : PROGRESS: at 56.77% words, alpha 0.00500, 3365 words/s\n",
      "2019-04-30 03:01:58,958 : INFO : PROGRESS: at 57.20% words, alpha 0.00500, 3366 words/s\n",
      "2019-04-30 03:02:19,119 : INFO : PROGRESS: at 57.62% words, alpha 0.00500, 3367 words/s\n",
      "2019-04-30 03:02:39,338 : INFO : PROGRESS: at 58.03% words, alpha 0.00500, 3368 words/s\n",
      "2019-04-30 03:02:59,561 : INFO : PROGRESS: at 58.45% words, alpha 0.00500, 3369 words/s\n",
      "2019-04-30 03:03:19,759 : INFO : PROGRESS: at 58.85% words, alpha 0.00500, 3368 words/s\n",
      "2019-04-30 03:03:39,903 : INFO : PROGRESS: at 59.25% words, alpha 0.00500, 3368 words/s\n",
      "2019-04-30 03:03:59,975 : INFO : PROGRESS: at 59.65% words, alpha 0.00500, 3368 words/s\n",
      "2019-04-30 03:04:20,015 : INFO : PROGRESS: at 60.06% words, alpha 0.00500, 3368 words/s\n",
      "2019-04-30 03:04:40,199 : INFO : PROGRESS: at 60.48% words, alpha 0.00500, 3369 words/s\n",
      "2019-04-30 03:05:00,214 : INFO : PROGRESS: at 60.90% words, alpha 0.00500, 3369 words/s\n",
      "2019-04-30 03:05:20,234 : INFO : PROGRESS: at 61.32% words, alpha 0.00500, 3370 words/s\n",
      "2019-04-30 03:05:40,382 : INFO : PROGRESS: at 61.75% words, alpha 0.00500, 3371 words/s\n",
      "2019-04-30 03:06:00,456 : INFO : PROGRESS: at 62.16% words, alpha 0.00500, 3372 words/s\n",
      "2019-04-30 03:06:20,497 : INFO : PROGRESS: at 62.59% words, alpha 0.00500, 3374 words/s\n",
      "2019-04-30 03:06:40,746 : INFO : PROGRESS: at 63.01% words, alpha 0.00500, 3374 words/s\n",
      "2019-04-30 03:07:00,920 : INFO : PROGRESS: at 63.44% words, alpha 0.00500, 3375 words/s\n",
      "2019-04-30 03:07:20,994 : INFO : PROGRESS: at 63.81% words, alpha 0.00500, 3373 words/s\n",
      "2019-04-30 03:07:41,104 : INFO : PROGRESS: at 64.19% words, alpha 0.00500, 3372 words/s\n",
      "2019-04-30 03:08:01,120 : INFO : PROGRESS: at 64.61% words, alpha 0.00500, 3373 words/s\n",
      "2019-04-30 03:08:21,317 : INFO : PROGRESS: at 65.02% words, alpha 0.00500, 3373 words/s\n",
      "2019-04-30 03:08:41,559 : INFO : PROGRESS: at 65.45% words, alpha 0.00500, 3374 words/s\n",
      "2019-04-30 03:09:01,634 : INFO : PROGRESS: at 65.88% words, alpha 0.00500, 3375 words/s\n",
      "2019-04-30 03:09:21,889 : INFO : PROGRESS: at 66.31% words, alpha 0.00500, 3377 words/s\n",
      "2019-04-30 03:09:41,891 : INFO : PROGRESS: at 66.73% words, alpha 0.00500, 3377 words/s\n",
      "2019-04-30 03:10:01,955 : INFO : PROGRESS: at 67.14% words, alpha 0.00500, 3378 words/s\n",
      "2019-04-30 03:10:22,177 : INFO : PROGRESS: at 67.56% words, alpha 0.00500, 3378 words/s\n",
      "2019-04-30 03:10:42,274 : INFO : PROGRESS: at 67.99% words, alpha 0.00500, 3379 words/s\n",
      "2019-04-30 03:11:02,531 : INFO : PROGRESS: at 68.41% words, alpha 0.00500, 3380 words/s\n",
      "2019-04-30 03:11:22,611 : INFO : PROGRESS: at 68.84% words, alpha 0.00500, 3381 words/s\n",
      "2019-04-30 03:11:42,885 : INFO : PROGRESS: at 69.28% words, alpha 0.00500, 3382 words/s\n",
      "2019-04-30 03:12:03,073 : INFO : PROGRESS: at 69.70% words, alpha 0.00500, 3383 words/s\n",
      "2019-04-30 03:12:23,166 : INFO : PROGRESS: at 70.09% words, alpha 0.00500, 3382 words/s\n",
      "2019-04-30 03:12:43,222 : INFO : PROGRESS: at 70.46% words, alpha 0.00500, 3380 words/s\n",
      "2019-04-30 03:13:03,288 : INFO : PROGRESS: at 70.87% words, alpha 0.00500, 3381 words/s\n",
      "2019-04-30 03:13:23,478 : INFO : PROGRESS: at 71.30% words, alpha 0.00500, 3382 words/s\n",
      "2019-04-30 03:13:43,697 : INFO : PROGRESS: at 71.74% words, alpha 0.00500, 3383 words/s\n",
      "2019-04-30 03:14:03,896 : INFO : PROGRESS: at 72.17% words, alpha 0.00500, 3384 words/s\n",
      "2019-04-30 03:14:23,899 : INFO : PROGRESS: at 72.59% words, alpha 0.00500, 3385 words/s\n",
      "2019-04-30 03:14:43,986 : INFO : PROGRESS: at 73.02% words, alpha 0.00500, 3386 words/s\n",
      "2019-04-30 03:15:04,037 : INFO : PROGRESS: at 73.41% words, alpha 0.00500, 3385 words/s\n",
      "2019-04-30 03:15:24,277 : INFO : PROGRESS: at 73.84% words, alpha 0.00500, 3386 words/s\n",
      "2019-04-30 03:15:44,485 : INFO : PROGRESS: at 74.26% words, alpha 0.00500, 3386 words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-30 03:16:04,703 : INFO : PROGRESS: at 74.68% words, alpha 0.00500, 3387 words/s\n",
      "2019-04-30 03:16:24,915 : INFO : PROGRESS: at 75.11% words, alpha 0.00500, 3388 words/s\n",
      "2019-04-30 03:16:45,144 : INFO : PROGRESS: at 75.54% words, alpha 0.00500, 3389 words/s\n",
      "2019-04-30 03:17:05,373 : INFO : PROGRESS: at 75.93% words, alpha 0.00500, 3388 words/s\n",
      "2019-04-30 03:17:25,517 : INFO : PROGRESS: at 76.25% words, alpha 0.00500, 3384 words/s\n",
      "2019-04-30 03:17:45,543 : INFO : PROGRESS: at 76.55% words, alpha 0.00500, 3380 words/s\n",
      "2019-04-30 03:18:05,911 : INFO : PROGRESS: at 76.83% words, alpha 0.00500, 3374 words/s\n",
      "2019-04-30 03:18:26,084 : INFO : PROGRESS: at 77.15% words, alpha 0.00500, 3370 words/s\n",
      "2019-04-30 03:18:46,403 : INFO : PROGRESS: at 77.53% words, alpha 0.00500, 3368 words/s\n",
      "2019-04-30 03:19:06,571 : INFO : PROGRESS: at 77.90% words, alpha 0.00500, 3367 words/s\n",
      "2019-04-30 03:19:26,754 : INFO : PROGRESS: at 78.28% words, alpha 0.00500, 3366 words/s\n",
      "2019-04-30 03:19:46,878 : INFO : PROGRESS: at 78.65% words, alpha 0.00500, 3364 words/s\n",
      "2019-04-30 03:20:06,935 : INFO : PROGRESS: at 79.00% words, alpha 0.00500, 3362 words/s\n",
      "2019-04-30 03:20:26,940 : INFO : PROGRESS: at 79.35% words, alpha 0.00500, 3360 words/s\n",
      "2019-04-30 03:20:47,119 : INFO : PROGRESS: at 79.71% words, alpha 0.00500, 3358 words/s\n",
      "2019-04-30 03:21:07,175 : INFO : PROGRESS: at 80.08% words, alpha 0.00500, 3356 words/s\n",
      "2019-04-30 03:21:27,487 : INFO : PROGRESS: at 80.44% words, alpha 0.00500, 3354 words/s\n",
      "2019-04-30 03:21:47,718 : INFO : PROGRESS: at 80.78% words, alpha 0.00500, 3352 words/s\n",
      "2019-04-30 03:22:07,784 : INFO : PROGRESS: at 81.12% words, alpha 0.00500, 3349 words/s\n",
      "2019-04-30 03:22:27,885 : INFO : PROGRESS: at 81.47% words, alpha 0.00500, 3347 words/s\n",
      "2019-04-30 03:22:48,115 : INFO : PROGRESS: at 81.83% words, alpha 0.00500, 3345 words/s\n",
      "2019-04-30 03:23:08,291 : INFO : PROGRESS: at 82.19% words, alpha 0.00500, 3344 words/s\n",
      "2019-04-30 03:23:28,597 : INFO : PROGRESS: at 82.61% words, alpha 0.00500, 3344 words/s\n",
      "2019-04-30 03:23:48,891 : INFO : PROGRESS: at 82.96% words, alpha 0.00500, 3342 words/s\n",
      "2019-04-30 03:24:09,109 : INFO : PROGRESS: at 83.31% words, alpha 0.00500, 3340 words/s\n",
      "2019-04-30 03:24:29,411 : INFO : PROGRESS: at 83.72% words, alpha 0.00500, 3340 words/s\n",
      "2019-04-30 03:24:49,492 : INFO : PROGRESS: at 84.07% words, alpha 0.00500, 3338 words/s\n",
      "2019-04-30 03:25:09,778 : INFO : PROGRESS: at 84.42% words, alpha 0.00500, 3336 words/s\n",
      "2019-04-30 03:25:30,064 : INFO : PROGRESS: at 84.78% words, alpha 0.00500, 3334 words/s\n",
      "2019-04-30 03:25:50,327 : INFO : PROGRESS: at 85.14% words, alpha 0.00500, 3332 words/s\n",
      "2019-04-30 03:26:10,537 : INFO : PROGRESS: at 85.50% words, alpha 0.00500, 3330 words/s\n",
      "2019-04-30 03:26:30,755 : INFO : PROGRESS: at 85.87% words, alpha 0.00500, 3329 words/s\n",
      "2019-04-30 03:26:50,961 : INFO : PROGRESS: at 86.25% words, alpha 0.00500, 3328 words/s\n",
      "2019-04-30 03:27:11,051 : INFO : PROGRESS: at 86.60% words, alpha 0.00500, 3327 words/s\n",
      "2019-04-30 03:27:31,157 : INFO : PROGRESS: at 86.97% words, alpha 0.00500, 3325 words/s\n",
      "2019-04-30 03:27:51,158 : INFO : PROGRESS: at 87.33% words, alpha 0.00500, 3324 words/s\n",
      "2019-04-30 03:28:11,300 : INFO : PROGRESS: at 87.70% words, alpha 0.00500, 3323 words/s\n",
      "2019-04-30 03:28:31,441 : INFO : PROGRESS: at 88.11% words, alpha 0.00500, 3323 words/s\n",
      "2019-04-30 03:28:51,721 : INFO : PROGRESS: at 88.49% words, alpha 0.00500, 3322 words/s\n",
      "2019-04-30 03:29:12,054 : INFO : PROGRESS: at 88.83% words, alpha 0.00500, 3320 words/s\n",
      "2019-04-30 03:29:32,203 : INFO : PROGRESS: at 89.17% words, alpha 0.00500, 3318 words/s\n",
      "2019-04-30 03:29:52,355 : INFO : PROGRESS: at 89.52% words, alpha 0.00500, 3316 words/s\n",
      "2019-04-30 03:30:12,402 : INFO : PROGRESS: at 89.87% words, alpha 0.00500, 3314 words/s\n",
      "2019-04-30 03:30:32,590 : INFO : PROGRESS: at 90.27% words, alpha 0.00500, 3314 words/s\n",
      "2019-04-30 03:30:52,683 : INFO : PROGRESS: at 90.65% words, alpha 0.00500, 3313 words/s\n",
      "2019-04-30 03:31:12,906 : INFO : PROGRESS: at 91.05% words, alpha 0.00500, 3313 words/s\n",
      "2019-04-30 03:31:33,100 : INFO : PROGRESS: at 91.41% words, alpha 0.00500, 3312 words/s\n",
      "2019-04-30 03:31:53,141 : INFO : PROGRESS: at 91.81% words, alpha 0.00500, 3312 words/s\n",
      "2019-04-30 03:32:13,207 : INFO : PROGRESS: at 92.17% words, alpha 0.00500, 3310 words/s\n",
      "2019-04-30 03:32:33,367 : INFO : PROGRESS: at 92.56% words, alpha 0.00500, 3310 words/s\n",
      "2019-04-30 03:32:53,662 : INFO : PROGRESS: at 92.95% words, alpha 0.00500, 3310 words/s\n",
      "2019-04-30 03:33:13,683 : INFO : PROGRESS: at 93.35% words, alpha 0.00500, 3310 words/s\n",
      "2019-04-30 03:33:33,939 : INFO : PROGRESS: at 93.73% words, alpha 0.00500, 3309 words/s\n",
      "2019-04-30 03:33:54,269 : INFO : PROGRESS: at 94.11% words, alpha 0.00500, 3308 words/s\n",
      "2019-04-30 03:34:14,481 : INFO : PROGRESS: at 94.46% words, alpha 0.00500, 3307 words/s\n",
      "2019-04-30 03:34:34,627 : INFO : PROGRESS: at 94.80% words, alpha 0.00500, 3305 words/s\n",
      "2019-04-30 03:34:54,824 : INFO : PROGRESS: at 95.17% words, alpha 0.00500, 3303 words/s\n",
      "2019-04-30 03:35:15,079 : INFO : PROGRESS: at 95.55% words, alpha 0.00500, 3303 words/s\n",
      "2019-04-30 03:35:35,389 : INFO : PROGRESS: at 95.92% words, alpha 0.00500, 3302 words/s\n",
      "2019-04-30 03:35:55,437 : INFO : PROGRESS: at 96.30% words, alpha 0.00500, 3301 words/s\n",
      "2019-04-30 03:36:15,708 : INFO : PROGRESS: at 96.68% words, alpha 0.00500, 3300 words/s\n",
      "2019-04-30 03:36:35,921 : INFO : PROGRESS: at 97.07% words, alpha 0.00500, 3300 words/s\n",
      "2019-04-30 03:36:56,066 : INFO : PROGRESS: at 97.46% words, alpha 0.00500, 3300 words/s\n",
      "2019-04-30 03:37:16,082 : INFO : PROGRESS: at 97.85% words, alpha 0.00500, 3300 words/s\n",
      "2019-04-30 03:37:36,317 : INFO : PROGRESS: at 98.29% words, alpha 0.00500, 3301 words/s\n",
      "2019-04-30 03:37:56,539 : INFO : PROGRESS: at 98.73% words, alpha 0.00500, 3303 words/s\n",
      "2019-04-30 03:38:16,590 : INFO : PROGRESS: at 99.22% words, alpha 0.00500, 3306 words/s\n",
      "2019-04-30 03:38:36,728 : INFO : PROGRESS: at 99.77% words, alpha 0.00500, 3311 words/s\n",
      "2019-04-30 03:38:45,654 : INFO : training on 16718844 words took 5047.3s, 3312 words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the word2vec model on multiple iterations ....\n",
      "PROGRESS: at sentence #0, processed 0 words and 0 unique words\n",
      "PROGRESS: at sentence #1000, processed 1000000 words and 52754 unique words\n",
      "PROGRESS: at sentence #2000, processed 2000000 words and 78382 unique words\n",
      "PROGRESS: at sentence #3000, processed 3000000 words and 96644 unique words\n",
      "PROGRESS: at sentence #4000, processed 4000000 words and 111460 unique words\n",
      "PROGRESS: at sentence #5000, processed 5000000 words and 125354 unique words\n",
      "PROGRESS: at sentence #6000, processed 6000000 words and 139565 unique words\n",
      "PROGRESS: at sentence #7000, processed 7000000 words and 151933 unique words\n",
      "PROGRESS: at sentence #8000, processed 8000000 words and 164114 unique words\n",
      "PROGRESS: at sentence #9000, processed 9000000 words and 178162 unique words\n",
      "PROGRESS: at sentence #10000, processed 10000000 words and 189074 unique words\n",
      "PROGRESS: at sentence #11000, processed 11000000 words and 198757 unique words\n",
      "PROGRESS: at sentence #12000, processed 12000000 words and 207894 unique words\n",
      "PROGRESS: at sentence #13000, processed 13000000 words and 217127 unique words\n",
      "PROGRESS: at sentence #14000, processed 14000000 words and 226854 unique words\n",
      "PROGRESS: at sentence #15000, processed 15000000 words and 237390 unique words\n",
      "PROGRESS: at sentence #16000, processed 16000000 words and 245648 unique words\n",
      "PROGRESS: at sentence #17000, processed 17000000 words and 253833 unique words\n",
      "collected 253854 unique words from a corpus of 17005207 words and 17006 sentences\n",
      "PROGRESS: at sentence #0\n",
      "PROGRESS: at sentence #1000\n",
      "PROGRESS: at sentence #2000\n",
      "PROGRESS: at sentence #3000\n",
      "PROGRESS: at sentence #4000\n",
      "PROGRESS: at sentence #5000\n",
      "PROGRESS: at sentence #6000\n",
      "PROGRESS: at sentence #7000\n",
      "PROGRESS: at sentence #8000\n",
      "PROGRESS: at sentence #9000\n",
      "PROGRESS: at sentence #10000\n",
      "PROGRESS: at sentence #11000\n",
      "PROGRESS: at sentence #12000\n",
      "PROGRESS: at sentence #13000\n",
      "PROGRESS: at sentence #14000\n",
      "PROGRESS: at sentence #15000\n",
      "PROGRESS: at sentence #16000\n",
      "PROGRESS: at sentence #17000\n",
      "PROGRESS: through with all the sentences\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bed504ed947e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontext2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaveInterm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cbow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/csci548-project1/src/conec/examples/conec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_all, seed, iterations, saveInterm, modelType, embed_dim, minCount, window, hs, neg, thr, alpha, min_alpha, forward, backward, progress, fill_diag, normalize)\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mcontext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContextModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0mcontext_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_diag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_diag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci548-project1/src/conec/examples/context2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, min_count, window, forward, backward, wordlist, progress)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_raw_context_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_windex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci548-project1/src/conec/examples/context2vec.py\u001b[0m in \u001b[0;36m_get_raw_context_matrix\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     84\u001b[0m                                 \u001b[0mfeatmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m  \u001b[0;31m# /j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PROGRESS: through with all the sentences\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_context_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_diag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36masformat\u001b[0;34m(self, format, copy)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# Forward the copy kwarg, if it's accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36mtocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context2vec.train(iterations=1, saveInterm=False, modelType=\"cbow\", embed_dim=200 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the embedding of a word\n",
    "---\n",
    "Use the model trained above to get the embeddings for any word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06313061, -0.07602396, -0.05714705,  0.00653351, -0.11048223,\n",
       "        0.0252326 , -0.02434733, -0.02137714, -0.01423272,  0.06427327,\n",
       "       -0.03336669, -0.07254225,  0.00441391,  0.01748119,  0.07599923,\n",
       "       -0.01571278,  0.05168891, -0.0277834 ,  0.03054369, -0.10963402,\n",
       "        0.09322368,  0.03089322,  0.03466579,  0.07251599, -0.09093159,\n",
       "       -0.03380995, -0.02431794, -0.00702254, -0.05796552, -0.00326498,\n",
       "        0.04588396,  0.19899118,  0.03095079,  0.00246658, -0.01261412,\n",
       "        0.00416606,  0.06471258, -0.09287103,  0.015593  , -0.07240081,\n",
       "       -0.10006761, -0.03702702,  0.10057703,  0.08622302,  0.05144106,\n",
       "       -0.12438264, -0.03972326,  0.10357558, -0.06007904, -0.057866  ,\n",
       "        0.07848126, -0.05514053,  0.00301895,  0.05253316,  0.1047078 ,\n",
       "       -0.07706435, -0.01753215, -0.14088754, -0.05128637, -0.11780416,\n",
       "       -0.02187855, -0.10276379, -0.06722963, -0.11287896, -0.04223928,\n",
       "       -0.04004713, -0.08583377, -0.07594945, -0.10564983, -0.03226064,\n",
       "       -0.11751023, -0.00484641, -0.06960086, -0.0505131 ,  0.0739442 ,\n",
       "        0.02756891,  0.09570383, -0.05609953, -0.1078475 ,  0.09445224,\n",
       "        0.05819346,  0.0150283 ,  0.02022685, -0.06425314,  0.08358288,\n",
       "       -0.0374501 ,  0.03520033,  0.04560691, -0.01206816, -0.14413554,\n",
       "       -0.02237437, -0.10817587,  0.10190717, -0.00849659, -0.16171754,\n",
       "        0.06981412,  0.10947972,  0.01469322,  0.06215682, -0.10279723,\n",
       "       -0.07709504, -0.03571252, -0.079325  ,  0.09943841,  0.00933648,\n",
       "        0.00871228, -0.03966363, -0.02529286, -0.00640285,  0.13604556,\n",
       "        0.04015232, -0.05685707,  0.0466617 ,  0.000705  , -0.01693722,\n",
       "       -0.15223139,  0.02937594,  0.00973815, -0.04457686,  0.06077067,\n",
       "        0.07606178, -0.10566285, -0.05083328, -0.09616166, -0.08247097,\n",
       "        0.01606898,  0.03205292,  0.02571818,  0.05658889,  0.02941924,\n",
       "        0.12030774,  0.01490227, -0.06992301,  0.06918033, -0.03135783,\n",
       "       -0.0370589 ,  0.06390703,  0.0408817 ,  0.03772045, -0.06922143,\n",
       "        0.13807612,  0.12149465,  0.01054071,  0.06437526, -0.0918709 ,\n",
       "        0.07331808, -0.03206617,  0.16168539,  0.02379359,  0.00428518,\n",
       "       -0.11273449,  0.06554997,  0.00636664, -0.00516382,  0.07381702,\n",
       "       -0.06486062,  0.03398852, -0.05367231, -0.00118092,  0.11483612,\n",
       "       -0.00534018,  0.04389466, -0.08819591,  0.00314141, -0.02449708,\n",
       "        0.00033611,  0.04707423,  0.03820303,  0.07132561, -0.01852675,\n",
       "       -0.01448384,  0.01804169, -0.130349  ,  0.08626685, -0.1354799 ,\n",
       "       -0.04851371,  0.16644652,  0.13472653,  0.01424822, -0.07837267,\n",
       "        0.0117026 ,  0.03206523, -0.0359654 , -0.07132739,  0.06533741,\n",
       "        0.03692822, -0.1096719 , -0.03672294, -0.04952874, -0.0483257 ,\n",
       "       -0.09665819,  0.00610087,  0.10096755,  0.07471542, -0.0382572 ,\n",
       "        0.10464293, -0.07897483,  0.01395764, -0.00799213, -0.02313989])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context2vec.predict_embedding(\"student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the average embedding of a sentence\n",
    "---\n",
    "Use the model trained above to predict the embedding of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01163509, -0.07884874,  0.00262184, -0.09123751, -0.06797978,\n",
       "        0.05508037,  0.10401928, -0.0891546 , -0.0365746 , -0.11399917,\n",
       "       -0.02391525, -0.12577286, -0.07804844,  0.04706455,  0.00269985,\n",
       "        0.11023757, -0.02244621, -0.17133969,  0.10515701,  0.1027593 ,\n",
       "       -0.02952521, -0.00150653, -0.01417139,  0.0075647 ,  0.09811858,\n",
       "       -0.02553854, -0.03945464,  0.05370576, -0.09607009,  0.04525451,\n",
       "        0.05001477,  0.12098357,  0.04369219, -0.02442538, -0.04528957,\n",
       "        0.00766742,  0.05725306, -0.00101892,  0.082896  , -0.06109586,\n",
       "       -0.06224906, -0.00975781,  0.06704762,  0.07212045,  0.06790268,\n",
       "        0.00537759, -0.03744291,  0.10451163, -0.01319254, -0.01651715,\n",
       "       -0.00783649, -0.08851766, -0.01789069, -0.02096957,  0.12992635,\n",
       "       -0.0729432 , -0.12817314, -0.05297756, -0.01879428, -0.02234147,\n",
       "        0.05093659, -0.09001711, -0.15655198, -0.06050745, -0.12529044,\n",
       "       -0.04523863, -0.0414156 ,  0.00205496, -0.07549787,  0.056475  ,\n",
       "        0.00413063, -0.03577061, -0.02106063,  0.00748762,  0.02664361,\n",
       "        0.17216866,  0.11359013, -0.02178444, -0.08181191,  0.07447809,\n",
       "        0.0264104 ,  0.05707304,  0.06943721,  0.032346  , -0.03018301,\n",
       "        0.0444203 ,  0.02704129,  0.02130696,  0.09734209, -0.01995535,\n",
       "       -0.02839459, -0.0630017 ,  0.14696075,  0.00184759,  0.15836233,\n",
       "       -0.06665295, -0.09101194,  0.03465184,  0.00934063, -0.03153287,\n",
       "        0.05404058, -0.01051604, -0.03481729,  0.10821537,  0.09656854,\n",
       "       -0.03876763, -0.01060225, -0.01775077, -0.05735947, -0.06034209,\n",
       "       -0.06499376, -0.0543676 ,  0.05705721, -0.02230043,  0.01640758,\n",
       "       -0.01722449,  0.05636657, -0.01193616,  0.03400401,  0.05405865,\n",
       "        0.05188201, -0.10314845,  0.06673193, -0.09296054, -0.01269203,\n",
       "       -0.0014519 ,  0.05790378,  0.01651512,  0.01058007, -0.05353403,\n",
       "        0.0349251 ,  0.0174278 , -0.00375735,  0.11109596, -0.12789165,\n",
       "       -0.04142868,  0.07017939,  0.10072582, -0.07088861, -0.05838952,\n",
       "        0.1247324 , -0.01284092,  0.00820226, -0.05222325, -0.1277565 ,\n",
       "       -0.01978707,  0.0471506 , -0.02135303, -0.01152744, -0.09097991,\n",
       "        0.00479565, -0.01079822, -0.02496681, -0.00400358, -0.10544336,\n",
       "       -0.0340531 ,  0.11387215,  0.08099784,  0.07922064,  0.03373132,\n",
       "        0.0750787 ,  0.08569363, -0.08974512, -0.13532784,  0.02671861,\n",
       "       -0.00638875,  0.15805944, -0.05264279,  0.07305421,  0.02101242,\n",
       "        0.00289544, -0.12195097, -0.09834971,  0.08772851, -0.01722517,\n",
       "       -0.02931902,  0.14275244,  0.08006689,  0.05783516, -0.00587037,\n",
       "        0.01088237, -0.0548996 , -0.07232355, -0.00421297,  0.1107145 ,\n",
       "       -0.03154994,  0.0525261 , -0.04134776, -0.03295856, -0.05641306,\n",
       "       -0.22430909,  0.01660031,  0.06052173,  0.08738964, -0.01129356,\n",
       "        0.043742  ,  0.05153003, -0.17399411,  0.05734576,  0.02194669])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context2vec.predict_sent_embedding(\"This is a great class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity between words using their embeddings\n",
    "---\n",
    "Obtain the similarity between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6506592455816812"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context2vec.predict_similarity(\"wars\", \"war\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity between sentences using their average embeddings\n",
    "---\n",
    "Obtain the similarity between two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9734589682087043"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context2vec.predict_sent_similarity(\"I am a girl\", \"I am a woman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on Google Analogy Dataset\n",
    "---\n",
    "Evaluate the trained model on Google Analogy Dataset\n",
    "\n",
    "Ensure questions-words.txt is present in the data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bc17d70e39f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontext2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_analogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/csci548-project1/src/conec/examples/conec.py\u001b[0m in \u001b[0;36mevaluate_analogy\u001b[0;34m(self, model_path, output_path, input_path, restrict_vocab)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/questions-words.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mok_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "context2vec.evaluate_analogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on CoNLL 2003 NER Task\n",
    "---\n",
    "Evaluate the trained model on CoNLL 2003 NER Task\n",
    "\n",
    "Ensure train.txt, testa.txt, testb.txt are present in the data/conll2003 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-059756c8838c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontext2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/csci548-project1/src/conec/examples/conec.py\u001b[0m in \u001b[0;36mevaluate_ner\u001b[0;34m(self, model_path, input_path, output_path)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0;34m\"ner\"\u001b[0m \u001b[0mtask\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtest\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrained\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstored\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdefined\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \"\"\"\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "context2vec.evaluate_ner()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
